This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-03-20T03:38:20.603Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
dev/
  build.R
  process_file.R
  sync.R
man/
  checkColumnPrefix.Rd
  checkInterviewAge.Rd
  checkQualtricsDuplicates.Rd
  cleanDataFrameExists.Rd
  dataRequest.Rd
  getMongo.Rd
  getQualtrics.Rd
  getRedcap.Rd
  getSurvey.Rd
  getTask.Rd
  ndaCheckQualtricsDuplicates.Rd
  ndaRequest.Rd
  ndaRequiredVariablesExist.Rd
  wizaRdry-package.Rd
R/
  addPrefixToColumns.R
  aliases.R
  animations.R
  checkColumnPrefix.R
  checkInterviewAge.R
  checkQualtricsDuplicates.R
  cleanDataFrameExists.R
  ConfigEnv.R
  dataRequest.R
  dataWizardry-package.R
  getMongo.R
  getQualtrics.R
  getRedcap.R
  globals.R
  ndaCheckQualtricsDuplicates.R
  ndaRequest.R
  ndaRequiredVariablesExist.R
  ndaValidator.R
  processCaprData.R
  SecretsEnv.R
  testSuite.R
  zzz.R
.gitignore
.Rbuildignore
dataWizardry.Rproj
DESCRIPTION
LICENSE
LICENSE.md
NAMESPACE
README.md

================================================================
Repository Files
================================================================

================
File: dev/build.R
================
usethis::use_package("config")
usethis::use_package("qualtRics")
usethis::use_package("dplyr")
usethis::use_package("knitr")
usethis::use_package("REDCapR")
usethis::use_package("cli")
usethis::use_package("R6")
usethis::use_package("mongolite")
usethis::use_package("future")
usethis::use_package("future.apply")
usethis::use_package("parallel")
usethis::use_package("stringdist")
usethis::use_package("rlang")
usethis::use_package("httr")
usethis::use_package("jsonlite")


# syncs all functions from dev (belieflab/api) to wizaRdry
source("dev/sync.R")

# Generate documentation from roxygen comments
devtools::document()

# Load the package with the new name
devtools::load_all()

# Check if your package passes CRAN checks
devtools::check()

# Build the package
devtools::build()

# Install the package locally
devtools::install()

# run test cases
measure_1 <- wizaRdry::getRedcap("measure_1")
rgpts <- wizaRdry::getQualtrics("rgpts")
dd <- wizaRdry::getMongo("dd", "capr")

wizaRdry::dataRequest("measure1")

wizaRdry::ndaRequest("eefrt01")

# !!! remove previous version !!!
remove.packages("wizaRdry")

================
File: dev/process_file.R
================
# Copy file with content modification
process_file <- function(source_path, dest_path) {
  # Read the original file
  lines <- readLines(source_path)

  # Process each line - comment out lines with api/ references and library/require calls
  modified_lines <- sapply(lines, function(line) {
    if (grepl("^\\s*api/", line) ||
        grepl("source\\([\"']api/", line) ||
        grepl("list\\.files\\([\"']api/", line) ||
        grepl("\\bapi/", line) ||
        grepl("if\\s*\\(!require\\(", line) ||
        grepl("library\\(", line) ||
        grepl("require\\(", line)) {
      return(paste0("# ", line))  # Comment out the line
    } else {
      return(line)  # Keep line as is
    }
  })

  # Write to new location
  writeLines(modified_lines, dest_path)
  message(sprintf("File copied from %s to %s with api/ references and library calls commented out",
                  source_path, dest_path))
}

================
File: dev/sync.R
================
source("dev/process_file.R")

process_file("../api/getRedcap.R", "R/getRedcap.R")

file.copy(from = "../api/SecretsEnv.R", to = "R", overwrite = TRUE)
file.copy(from = "../api/ConfigEnv.R", to = "R", overwrite = TRUE)

file.copy(from = "../api/src/shortcuts.R", to = "R/aliases.R", overwrite = TRUE)
file.copy(from = "../api/src/animations.R", to = "R/animations.R", overwrite = TRUE)

process_file("../api/getSurvey.R", "R/getQualtrics.R")
process_file("../api/getTask.R", "R/getMongo.R")
file.copy(from = "../api/src/addPrefixToColumns.R", to = "R/addPrefixToColumns.R", overwrite = TRUE)


file.copy(from = "../api/redcap/capr-logic.R", to = "R/processCaprData.R", overwrite = TRUE)



process_file("../api/dataRequest.R", "R/dataRequest.R")

process_file("../api/ndaValidator.R", "R/ndaValidator.R")

process_file("../api/ndaRequest.R", "R/ndaRequest.R")

process_file("../api/testSuite.R", "R/testSuite.R")

process_file("../api/test/ndaCheckQualtricsDuplicates.R", "R/ndaCheckQualtricsDuplicates.R")

process_file("../api/test/checkQualtricsDuplicates.R", "R/checkQualtricsDuplicates.R")

process_file("../api/test/cleanDataFrameExists.R", "R/cleanDataFrameExists.R")

process_file("../api/test/ndaRequiredVariablesExist.R", "R/ndaRequiredVariablesExist.R")

process_file("../api/test/checkColumnPrefix.R", "R/checkColumnPrefix.R")

process_file("../api/test/checkInterviewAge.R", "R/checkInterviewAge.R")

================
File: man/checkColumnPrefix.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/checkColumnPrefix.R
\name{checkColumnPrefix}
\alias{checkColumnPrefix}
\title{Check Column Naming Conventions Against NDA Requirements}
\usage{
checkColumnPrefix(measure_alias, measure_type, nda_required_variables)
}
\arguments{
\item{measure_alias}{A string representing the prefix expected for non-NDA columns.}

\item{measure_type}{A string specifying the type of dataset, supports 'qualtrics' or 'redcap'.}

\item{nda_required_variables}{A vector of strings listing the NDA required variables which are excluded from the naming convention check.}
}
\value{
The function does not return a value but outputs an error message if any non-NDA columns do not follow the naming convention.
}
\description{
This function checks that all non-NDA columns in a dataset follow a specified naming convention,
prefixed with the measure alias. It is designed to support different types of datasets like Qualtrics and REDCap.
}
\note{
This function assumes that the dataset follows a specific naming convention where non-NDA columns should be prefixed with the measure alias followed by an underscore.
The actual dataset name is expected to be the measure alias suffixed with '_clean'.
}
\examples{
checkColumnPrefix("dataset_name", "qualtrics", c("nda_var1", "nda_var2"))
}

================
File: man/checkInterviewAge.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/checkInterviewAge.R
\name{checkInterviewAge}
\alias{checkInterviewAge}
\title{Check Interview Age Validity}
\usage{
checkInterviewAge(measure_alias)
}
\arguments{
\item{measure_alias}{A string representing the name of the dataset to check.
The dataset should have a column 'interview_age' and 'src_subject_id'.}
}
\value{
The function itself does not return a value but will output a message listing the subject IDs
with 'interview_age' outside the 12 to 70 years range if such cases exist.
}
\description{
This function checks whether the 'interview_age' for all records in a specified dataset
falls within the acceptable age range (12 to 70 years, converted into months).
If any record falls outside this range, the function will list the subject IDs that do not meet this condition.
}
\note{
This function requires the 'testthat' package for unit testing.
It is designed to operate on datasets that follow a specific naming convention,
appending '_clean' to the measure_alias to construct the dataframe name.
The function throws an error if the 'interview_age' falls outside the specified range.
}
\examples{
checkInterviewAge("your_dataset_alias")
}

================
File: man/checkQualtricsDuplicates.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/checkQualtricsDuplicates.R
\name{checkQualtricsDuplicates}
\alias{checkQualtricsDuplicates}
\title{Check for Duplicates in Qualtrics Data}
\usage{
checkQualtricsDuplicates(measure_alias, measure_type)
}
\arguments{
\item{measure_alias}{A string representing the name of the dataset to check for duplicates.}

\item{measure_type}{A string specifying the type of measure, currently supports 'qualtrics' only.}
}
\value{
The function does not return a value but will generate a CSV file if duplicates are found
and display those duplicates in the RStudio viewer.
}
\description{
This function checks for duplicate records in a specified Qualtrics dataset
based on certain identifiers and time points. If duplicates are found,
they are exported to a CSV file for review and additionally displayed in the viewer.
}
\note{
This function requires the dplyr and testthat packages. It is specifically designed for
Qualtrics data and expects the data frame to be named with a '_clean' suffix.
It checks for duplicates based on 'src_subject_id' combined with 'visit' or 'week' columns.
The function will stop and throw an error if the necessary columns are not present.
}
\examples{
checkDuplicates("your_dataset_alias", "qualtrics")
}

================
File: man/cleanDataFrameExists.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cleanDataFrameExists.R
\name{cleanDataFrameExists}
\alias{cleanDataFrameExists}
\title{Check if a Clean Data Frame Exists}
\usage{
cleanDataFrameExists(measure_alias, measure_type)
}
\arguments{
\item{measure_alias}{A string representing the alias name of the dataset to be checked.}

\item{measure_type}{A string indicating the type of measure, currently not utilized in the function but reserved for future use.}
}
\value{
This function does not return a value but outputs a message indicating whether the specified clean data frame exists.
}
\description{
This function checks if a cleaned data frame, specified by the measure alias, exists in the global environment.
The function is designed to verify the existence of data frames intended to have been cleaned and prepared
under a specific naming convention (suffix '_clean').
}
\note{
This function assumes that the dataset, if cleaned and prepared correctly, has been named according to a standard
naming convention with a '_clean' suffix. The measure_type parameter is included for potential future functionality
but is not currently used.
}
\examples{
cleanDataFrameExists("your_dataset_alias", "qualtrics")
}

================
File: man/dataRequest.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dataRequest.R
\name{dataRequest}
\alias{dataRequest}
\title{Data Request}
\usage{
dataRequest(..., csv = FALSE, rdata = FALSE, spss = FALSE)
}
\arguments{
\item{...}{Strings, specifying the measures to process, which can be a Mongo collection, REDCap instrument, or Qualtrics survey.}

\item{csv}{Optional; Boolean, if TRUE creates a .csv extract in ./tmp.}

\item{rdata}{Optional; Boolean, if TRUE creates an .rdata extract in ./tmp.}

\item{spss}{Optional; Boolean, if TRUE creates a .sav extract in ./tmp.}
}
\value{
Prints the time taken for the data request process.
}
\description{
This function processes requests for clean data sequentially for specified measures.
It makes a request to the appropriate API for the named measure or measures
and runs the associated data cleaning routines. It then runs a series of
unit tests to verify that the data quality standards are met.
}
\examples{
\dontrun{
  dataRequest("prl", csv=TRUE)
  dataRequest("rgpts", "kamin", rdata=TRUE)
}

}
\author{
Joshua Kenney \href{mailto:joshua.kenney@yale.edu}{joshua.kenney@yale.edu}
}

================
File: man/getMongo.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/getMongo.R
\name{getMongo}
\alias{getMongo}
\title{Retrieve data from MongoDB}
\usage{
getMongo(collection_name, db_name = NULL, identifier = NULL, chunk_size = NULL)
}
\arguments{
\item{collection_name}{The name of the MongoDB collection}

\item{db_name}{The database name (optional)}

\item{identifier}{Field to use as identifier (optional)}

\item{chunk_size}{Number of records per chunk (optional)}
}
\value{
A data frame containing the MongoDB data
}
\description{
Retrieve data from MongoDB
}

================
File: man/getQualtrics.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/getQualtrics.R
\name{getQualtrics}
\alias{getQualtrics}
\title{Retrieve Survey Data from Qualtrics}
\usage{
getQualtrics(qualtrics_alias, institution = NULL, label = FALSE)
}
\arguments{
\item{qualtrics_alias}{The alias for the Qualtrics survey to be retrieved.}

\item{institution}{Optional. The institution name (e.g., "temple" or "nu"). If NULL, all institutions will be searched.}

\item{label}{Logical indicating whether to return coded values or their associated labels (default is FALSE).}
}
\value{
A cleaned and harmonized data frame containing the survey data.
}
\description{
Retrieve Survey Data from Qualtrics
}
\examples{
\dontrun{
# Get survey by alias (will search all institutions)
survey_data <- getQualtrics("rgpts")
}
}

================
File: man/getRedcap.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/getRedcap.R
\name{getRedcap}
\alias{getRedcap}
\title{Get Data from REDCap}
\usage{
getRedcap(
  instrument_name = NULL,
  raw_or_label = "raw",
  redcap_event_name = NULL,
  batch_size = 1000,
  records = NULL,
  fields = NULL
)
}
\arguments{
\item{instrument_name}{Name of the REDCap instrument}

\item{raw_or_label}{Whether to return raw or labeled values}

\item{redcap_event_name}{Optional event name filter}

\item{batch_size}{Number of records to retrieve per batch}

\item{records}{Optional vector of specific record IDs}

\item{fields}{Optional vector of specific fields}
}
\value{
A data frame containing the requested REDCap data
}
\description{
Retrieves data from a REDCap instrument
}

================
File: man/getSurvey.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/getQualtrics.R
\name{getSurvey}
\alias{getSurvey}
\title{Alias for 'getQualtrics'}
\usage{
getSurvey(qualtrics_alias, institution = NULL, label = FALSE)
}
\arguments{
\item{qualtrics_alias}{The alias for the Qualtrics survey to be retrieved.}

\item{institution}{Optional. The institution name (e.g., "temple" or "nu"). If NULL, all institutions will be searched.}

\item{label}{Logical indicating whether to return coded values or their associated labels (default is FALSE).}
}
\value{
A cleaned and harmonized data frame containing the survey data.
}
\description{
This is a legacy alias for the 'getQualtrics' function to maintain compatibility with older code.
}
\examples{
\dontrun{
survey_data <- getSurvey("your_survey_alias")
}
}

================
File: man/getTask.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/getMongo.R
\name{getTask}
\alias{getTask}
\title{Alias for 'getMongo'}
\usage{
getTask(collection_name, db_name = NULL, identifier = NULL, chunk_size = NULL)
}
\arguments{
\item{collection_name}{The name of the MongoDB collection}

\item{db_name}{The database name (optional)}

\item{identifier}{Field to use as identifier (optional)}

\item{chunk_size}{Number of records per chunk (optional)}
}
\value{
A data frame containing the MongoDB data
}
\description{
This is a legacy alias for the 'getMongo' function to maintain compatibility with older code.
}
\examples{
\dontrun{
survey_data <- getTask("task_alias")
}
}

================
File: man/ndaCheckQualtricsDuplicates.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ndaCheckQualtricsDuplicates.R
\name{ndaCheckQualtricsDuplicates}
\alias{ndaCheckQualtricsDuplicates}
\title{Check for Duplicates in Qualtrics Data}
\usage{
ndaCheckQualtricsDuplicates(measure_alias, measure_type)
}
\arguments{
\item{measure_alias}{A string representing the name of the dataset to check for duplicates.}

\item{measure_type}{A string specifying the type of measure, currently supports 'qualtrics' only.}
}
\value{
The function does not return a value but will generate a CSV file if duplicates are found
and display those duplicates in the RStudio viewer.
}
\description{
This function checks for duplicate records in a specified Qualtrics dataset
based on certain identifiers and time points. If duplicates are found,
they are exported to a CSV file for review and additionally displayed in the viewer.
}
\note{
This function requires the dplyr and testthat packages. It is specifically designed for
Qualtrics data and expects the data frame to be named with a '_clean' suffix.
It checks for duplicates based on 'src_subject_id' combined with 'visit' or 'week' columns.
The function will stop and throw an error if the necessary columns are not present.
}
\examples{
checkDuplicates("your_dataset_alias", "qualtrics")
}

================
File: man/ndaRequest.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ndaRequest.R
\name{ndaRequest}
\alias{ndaRequest}
\title{NDA Request}
\usage{
ndaRequest(
  ...,
  csv = FALSE,
  rdata = FALSE,
  spss = FALSE,
  limited_dataset = FALSE
)
}
\arguments{
\item{...}{Strings, specifying the measures to process, which can be a Mongo collection, REDCap instrument, or Qualtrics survey.}

\item{csv}{Optional; Boolean, if TRUE creates a .csv extract in ./tmp.}

\item{rdata}{Optional; Boolean, if TRUE creates an .rdata extract in ./tmp.}

\item{spss}{Optional; Boolean, if TRUE creates a .sav extract in ./tmp.}

\item{limited_dataset}{Optional; Boolean, if TRUE does not perform date-shifting of interview_date or age-capping of interview_age}
}
\value{
Prints the time taken for the data request process.
}
\description{
This function processes requests for clean data sequentially for specified measures.
It makes a request to the NIH NDA API for the named data structures
and runs the associated data remediation routines. It then runs a series of
unit tests to verify that the data quality standards are met.
}
\examples{
\dontrun{
  ndaRequest("prl", csv=TRUE)
  ndaRequest("rgpts", "kamin", rdata=TRUE)
}

}
\author{
Joshua Kenney \href{mailto:joshua.kenney@yale.edu}{joshua.kenney@yale.edu}
}

================
File: man/ndaRequiredVariablesExist.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ndaRequiredVariablesExist.R
\name{ndaRequiredVariablesExist}
\alias{ndaRequiredVariablesExist}
\title{Check for Presence of NDA Required Variables in a Data Frame}
\usage{
ndaRequiredVariablesExist(measure_alias, measure_type, nda_required_variables)
}
\arguments{
\item{measure_alias}{A string representing the alias name of the dataset to be checked.}

\item{measure_type}{A string indicating the type of measure, used to adjust the list of required variables if necessary.}

\item{nda_required_variables}{A vector of strings representing the initial set of NDA required variables to check for.
This parameter is overwritten inside the function but can be used to extend the functionality in the future.}
}
\value{
This function does not return a value but uses the \code{testthat} package to assert the presence of all NDA required variables and provides detailed feedback if any are missing.
}
\description{
This function checks if a cleaned data frame contains all the variables required by the National Data Archive (NDA).
The set of required variables can be adjusted based on the specific requirements of the study and the presence
of certain variables like 'visit' or 'week'. The function is useful for ensuring data compliance before submission.
}
\note{
While currently the function overwrites the 'nda_required_variables' parameter internally, future versions may allow for dynamic adjustment based on 'measure_type'.
It assumes that the dataset has been cleaned and is named according to a standard naming convention with a '_clean' suffix.
}
\examples{
ndaRequiredVariablesExist("your_dataset_alias", "qualtrics", c("src_subject_id", "phenotype"))
}

================
File: man/wizaRdry-package.Rd
================
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dataWizardry-package.R
\docType{package}
\name{wizaRdry-package}
\alias{wizaRdry}
\alias{wizaRdry-package}
\title{wizaRdry: A Framework For Collaborative & Reproducable Data Analysis}
\description{
The wizaRdry package is a powerful data integration framework that solves the fundamental problem of inconsistent access to research data across multiple platforms. It eliminates repetitive boilerplate code by providing a unified interface to REDCap, MongoDB, and Qualtrics through its core functions: getRedcap(), getMongo(), and getQualtrics(). The package implements memory-aware parallel processing that automatically scales to available system resources, robust configuration validation, and comprehensive error handling with intelligent fallback strategies. By standardizing data retrieval across platforms and automating field harmonization, wizaRdry significantly reduces the risk of inconsistencies in multi-researcher environments. This package is essential for any research team working with distributed data sources who need reproducible, validated access to their research data.
}
\author{
\strong{Maintainer}: Joshua Kenney \email{joshua.kenney@yale.edu}

Authors:
\itemize{
  \item Minerva Pappu \email{minerva.pappu@yale.edu}
  \item Trevor Williams \email{trevormsu@gmail.com}
}

Other contributors:
\itemize{
  \item Victor Pokorny \email{victor.pokorny@northwestern.edu} [contributor]
  \item Christian Horgan \email{christian.horgan@yale.edu} [contributor]
  \item Michael Spilka \email{michael.j.spilka@gmail.com} [contributor]
}

}
\keyword{internal}

================
File: R/addPrefixToColumns.R
================
#' Add prefix to columns and handle text normalization
#'
#' @param df A data frame
#' @param collection_name The prefix to add
#' @param excluded_cols Columns to exclude from prefixing
#' @param similarity_threshold Threshold for considering a prefix similar (0-1)
#' @param normalize_case Whether to normalize column case (lowercase, etc)
#' @param normalize_type Type of case normalization ("lower", "upper", "title", etc)
#' @return A data frame with prefixed and normalized column names
#' @importFrom stringdist stringdist
#' @noRd
add_prefix_to_columns <- function(df, collection_name, 
                                  excluded_cols = c("src_subject_id", "visit", "phenotype", 
                                                    "interview_age", "interview_date", "site", 
                                                    "sex", "subjectkey", "arm", "handedness", 
                                                    "state", "status"),
                                  similarity_threshold = 0.8,
                                  normalize_case = FALSE,
                                  normalize_type = "lower") {
  
  # Ensure stringdist package is available
  if (!requireNamespace("stringdist", quietly = TRUE)) {
    stop("The stringdist package is required. Please install it with install.packages('stringdist')")
  }
  
  # Helper function to check if a column has a similar prefix
  has_similar_prefix <- function(col_name, prefix) {
    # If the column is too short, it can't have a meaningful prefix
    if (nchar(col_name) < nchar(prefix)) {
      return(FALSE)
    }
    
    # Extract potential prefix (same length as collection_name)
    potential_prefix <- substring(col_name, 1, nchar(prefix))
    
    # Calculate similarity
    similarity <- 1 - stringdist::stringdist(tolower(potential_prefix), 
                                             tolower(prefix), 
                                             method = "jw") / nchar(prefix)
    
    return(similarity >= similarity_threshold)
  }
  
  # Get the current column names
  col_names <- names(df)
  
  # Replace dots with underscores
  col_names <- gsub("\\.", "_", col_names)
  
  # Create a new set of column names
  new_col_names <- sapply(col_names, function(col) {
    # Check if column should be excluded from prefixing
    if (col %in% excluded_cols) {
      return(col)
    }
    
    # Check if the column already has the exact prefix
    if (startsWith(col, paste0(collection_name, "_"))) {
      return(col)
    }
    
    # Check if the column has a similar prefix using string distance
    if (has_similar_prefix(col, collection_name)) {
      return(col)  # Keep as is if it has a similar prefix
    }
    
    # Add the prefix if none of the above conditions are met
    return(paste0(collection_name, "_", col))
  })
  
  # Normalize case if requested
  if (normalize_case) {
    if (normalize_type == "lower") {
      new_col_names <- tolower(new_col_names)
    } else if (normalize_type == "upper") {
      new_col_names <- toupper(new_col_names)
    } else if (normalize_type == "title") {
      # Title case (capitalize first letter of each word)
      new_col_names <- gsub("(^|_)([a-z])", "\\1\\U\\2", tolower(new_col_names), perl = TRUE)
    }
  }
  
  # Rename the columns
  names(df) <- new_col_names
  
  return(df)
}

================
File: R/aliases.R
================
# negate in
`%!in%` = base::Negate(`%in%`)

================
File: R/animations.R
================
# Define the loading animation function
show_loading_animation <- function() {
  cat("Loading ")
  pb <- txtProgressBar(min = 0, max = 20, style = 3)
  
  for (i in 1:20) {
    Sys.sleep(0.1) # Simulate some computation time
    setTxtProgressBar(pb, i)
  }
  
  close(pb)
}

# Define the progress callback function
progress_callback <- function(count, total) {
  setTxtProgressBar(pb, count) # Update the loading animation
}

#' @noRd
initializeLoadingAnimation <- function(max_progress) {
  # cat("Loading...\n")
  pb <- txtProgressBar(min = 0, max = max_progress, style = 3)
  return(pb)
}

#' @noRd
updateLoadingAnimation <- function(pb, current_progress) {
  setTxtProgressBar(pb, current_progress)
}
#' @noRd
completeLoadingAnimation <- function(pb) {
  close(pb)
}


#' Initialize or Update Loading Animation
#'
#' Initializes a new loading animation or updates an existing one based on the progress.
#'
#' @param step Current step (chunk being processed).
#' @param total_steps Total number of chunks.
#' @param pb An existing txtProgressBar object; if NULL, a new progress bar is created.
#' @return A txtProgressBar object representing the current state of the progress bar.
#' @examples
#' \dontrun{
#' pb <- show_loading_animation_with_chunks(step = 1, total_steps = 20)  # Start new animation
#' pb <- show_loading_animation_with_chunks(step = 2, total_steps = 20, pb = pb)  # Update animation
#' }
#' @noRd
show_loading_animation_with_chunks <- function(step = NULL, total_steps = NULL, pb = NULL) {
  if (is.null(pb)) {
    # cat("Loading:\n")  # Comment this out if you don't want the static "Loading..." message
    pb <- txtProgressBar(min = 0, max = total_steps, style = 3)
  }
  if (!is.null(step) && !is.null(total_steps)) {
    setTxtProgressBar(pb, step)
  }
  return(pb)
}

================
File: R/checkColumnPrefix.R
================
#' Check Column Naming Conventions Against NDA Requirements
#'
#' This function checks that all non-NDA columns in a dataset follow a specified naming convention,
#' prefixed with the measure alias. It is designed to support different types of datasets like Qualtrics and REDCap.
#'
#' @param measure_alias A string representing the prefix expected for non-NDA columns.
#' @param measure_type A string specifying the type of dataset, supports 'qualtrics' or 'redcap'.
#' @param nda_required_variables A vector of strings listing the NDA required variables which are excluded from the naming convention check.
#' @return The function does not return a value but outputs an error message if any non-NDA columns do not follow the naming convention.
#' @export
#' @examples
#' checkColumnPrefix("dataset_name", "qualtrics", c("nda_var1", "nda_var2"))
#' @importFrom testthat test_that expect_true
#' @importFrom dplyr setdiff
#' @note This function assumes that the dataset follows a specific naming convention where non-NDA columns should be prefixed with the measure alias followed by an underscore.
#'       The actual dataset name is expected to be the measure alias suffixed with '_clean'.
checkColumnPrefix <- function(measure_alias, measure_type, nda_required_variables) {
  
#   if (!require(testthat)) {install.packages("testthat")}; library(testthat)
#   if (!require(dplyr)) {install.packages("dplyr")}; library(dplyr)
  
  # Construct dataframe name based on measure_alias
  df_name <- paste0(measure_alias, "_clean")
  
  # Fetch the dataframe
  df <- base::get(df_name)
  
  # Determine columns to check based on measure_type
  if (measure_type == "qualtrics") {
    non_nda_cols <- dplyr::setdiff(colnames(df), c(nda_required_variables, "ResponseId"))
  } else if (measure_type=="redcap") {
    non_nda_cols <- dplyr::setdiff(colnames(df), c(nda_required_variables, "int_start", 
                                            "int_end"))
  } else {
    non_nda_cols <- dplyr::setdiff(colnames(df), nda_required_variables)
  }
  
  tryCatch({
    # Check naming convention for non-NDA columns
    test_that("Check column naming convention", {
      actual_non_conform <- non_nda_cols[!grepl(paste0("^", measure_alias, "_"), non_nda_cols)]
      is_conforming <- length(actual_non_conform) == 0
      expect_true(
        is_conforming,
        info = paste("SCRIPT ERROR: The following non-NDA columns in '", df_name, 
                     "' do not follow the correct naming convention starting with '", measure_alias, "_':",
                     paste(actual_non_conform, collapse = ", "))
      )
    })
  }, error = function(e) {
    message(e$message)
  })
}

================
File: R/checkInterviewAge.R
================
#' Check Interview Age Validity
#'
#' This function checks whether the 'interview_age' for all records in a specified dataset
#' falls within the acceptable age range (12 to 70 years, converted into months).
#' If any record falls outside this range, the function will list the subject IDs that do not meet this condition.
#'
#' @param measure_alias A string representing the name of the dataset to check.
#'        The dataset should have a column 'interview_age' and 'src_subject_id'.
#' @return The function itself does not return a value but will output a message listing the subject IDs
#'         with 'interview_age' outside the 12 to 70 years range if such cases exist.
#' @export
#' @examples
#' checkInterviewAge("your_dataset_alias")
#' @importFrom testthat test_that expect_true
#' @note This function requires the 'testthat' package for unit testing.
#'       It is designed to operate on datasets that follow a specific naming convention,
#'       appending '_clean' to the measure_alias to construct the dataframe name.
#'       The function throws an error if the 'interview_age' falls outside the specified range.
checkInterviewAge <- function(measure_alias) {
  
#   if (!require(testthat)) {install.packages("testthat")}; library(testthat)
  
  months_in_a_year <- 12
  # define age range
  min_age <- 12 * months_in_a_year  # 144 months
  max_age <- 70 * months_in_a_year  # 840 months
  
  # Construct the expected dataframe name
  output_df_name <- paste0(measure_alias, "_clean")
  
  # Retrieve the dataframe based on constructed name
  df_clean <- base::get(output_df_name)  # specify the environment if needed
  
  # age checker
  rows_not_meeting_condition <- df_clean$src_subject_id[df_clean$interview_age < min_age | df_clean$interview_age > max_age]
  
  tryCatch({
    # Perform tests
    test_that(paste0("Check interview_age is between ", min_age, " and ", max_age), {
      testthat::expect_true(
        all(df_clean$interview_age >= min_age & df_clean$interview_age <= max_age),
        info = paste("DATA ERROR: All values in 'interview_age' should be greater than ", min_age, " and less than ", max_age, ". src_subject_id not meeting condition:", paste(rows_not_meeting_condition, collapse = ", "))
      )
    })
  }, error = function(e) {
    message("The following subjects have out of range ages:",rows_not_meeting_condition, e$message)
  })
}

================
File: R/checkQualtricsDuplicates.R
================
#' Check for Duplicates in Qualtrics Data
#'
#' This function checks for duplicate records in a specified Qualtrics dataset
#' based on certain identifiers and time points. If duplicates are found,
#' they are exported to a CSV file for review and additionally displayed in the viewer.
#'
#' @param measure_alias A string representing the name of the dataset to check for duplicates.
#' @param measure_type A string specifying the type of measure, currently supports 'qualtrics' only.
#' @return The function does not return a value but will generate a CSV file if duplicates are found
#'         and display those duplicates in the RStudio viewer.
#' @export
#' @examples
#' checkDuplicates("your_dataset_alias", "qualtrics")
#' @importFrom dplyr filter %>% 
#' @importFrom testthat test_that expect_true
#' @note This function requires the dplyr and testthat packages. It is specifically designed for
#'       Qualtrics data and expects the data frame to be named with a '_clean' suffix.
#'       It checks for duplicates based on 'src_subject_id' combined with 'visit' or 'week' columns.
#'       The function will stop and throw an error if the necessary columns are not present.
checkQualtricsDuplicates <- function(measure_alias, measure_type) {
  
  # Ensure required packages are loaded
#   if (!require(dplyr)) { install.packages("dplyr") }; library(dplyr)
#   if (!require(testthat)) { install.packages("testthat") }; library(testthat)
  
  # Generate the name of the dataframe and get it
  output_df_name <- paste0(measure_alias, "_clean")
  df <- base::get(output_df_name)
  
  identifier <- "src_subject_id"
  
  if (measure_type == "qualtrics") {
    if (!(identifier %in% colnames(df))) {
      stop("Please provide a valid identifier: src_subject_id, workerId, PROLIFIC_PID")
    }
    
    for (col in c("visit", "week")) {
      if (col %in% base::colnames(df)) {
        df$duplicates <- base::duplicated(df[c(identifier, col)], first = TRUE)
        df_dup_ids <- base::subset(df, duplicates == TRUE)[, c(identifier, col)]
        
        if (base::nrow(df_dup_ids) > 0) {
          # Using inner_join to filter duplicates correctly
          df_duplicates <- df %>% 
            dplyr::inner_join(df_dup_ids, by = c(identifier, col))

          if (base::nrow(df_duplicates) > 0) {
            # Export and create a CSV file if duplicates found
            duplicate_extract <- paste0("duplicates_", measure_alias)
            createCsv(df_duplicates, paste0("duplicates_", measure_alias))
            
            tryCatch({
              testthat::test_that("Check for Qualtrics duplicates", {
                testthat::expect_true(base::nrow(df_duplicates) == 0, 
                                      info = paste("DATA ERROR: Duplicates detected in '", measure_alias, "': ", 
                                                   "Offending IDs: ", toString(base::unique(df_duplicates[[identifier]]))))
              })
            }, error = function(e) {
              duplicates_summary <- toString(base::unique(df_duplicates[[identifier]]))
              message(paste("Error in testing for duplicates in '", measure_alias, "': ", e$message, 
                            "\nOffending IDs: ", duplicates_summary,
                            "\nDetails exported to ", paste0("./tmp/",duplicate_extract,".csv")))
            })
            
            # Optionally view the offending records in RStudio's data viewer
            # View(df_duplicates)
          }
        }
      }
    }
  }
}

================
File: R/cleanDataFrameExists.R
================
#' Check if a Clean Data Frame Exists
#'
#' This function checks if a cleaned data frame, specified by the measure alias, exists in the global environment.
#' The function is designed to verify the existence of data frames intended to have been cleaned and prepared
#' under a specific naming convention (suffix '_clean').
#'
#' @param measure_alias A string representing the alias name of the dataset to be checked.
#' @param measure_type A string indicating the type of measure, currently not utilized in the function but reserved for future use.
#' @return This function does not return a value but outputs a message indicating whether the specified clean data frame exists.
#' @export
#' @examples
#' cleanDataFrameExists("your_dataset_alias", "qualtrics")
#' @importFrom testthat test_that expect_true
#' @note This function assumes that the dataset, if cleaned and prepared correctly, has been named according to a standard
#'       naming convention with a '_clean' suffix. The measure_type parameter is included for potential future functionality
#'       but is not currently used.
cleanDataFrameExists <- function(measure_alias, measure_type) {
  
#   if (!require(testthat)) {install.packages("testthat")}; library(testthat)
  
  # append _clean to the measure in question
  output_df_name <- paste0(measure_alias, "_clean")
  
  tryCatch({
    
    test_that("Clean df exists", {
      
      # Check if the expected output_df_name is created
      testthat::expect_true(exists(output_df_name), 
                                               info = paste("SCRIPT ERROR: The script did not create '", output_df_name, "' dataframe."))
    })
    
  }, error = function(e) {
    message("The script did not create a'", output_df_name, "' dataframe.", e$message)
  })
  
}

================
File: R/ConfigEnv.R
================
# First, install R6 if you don't have it
if (!require(R6)) install.packages("R6"); library(R6)
# Install config package if needed
if (!require(config)) install.packages("config"); library(config)

#' Configuration Environment Class
#' 
#' @importFrom R6 R6Class
#' @noRd
ConfigEnv <- R6::R6Class("ConfigEnv",
                         public = list(
                           # Store configuration
                           config = NULL,
                           config_file = NULL,
                           
                           # Define validation specs for each API
                           api_specs = list(
                             mongo = list(
                               required = c("collection")
                             ),
                             qualtrics = list(
                               required = c("survey_ids")
                             ),
                             redcap = list(
                               required = c("super_keys")
                             ),
                             sql = list(
                               required = c()  # Add required fields for SQL as needed
                             )
                           ),
                           
                           initialize = function(config_file = "config.yml") {
                             # Check if config file exists
                             if (!file.exists(config_file)) {
                               stop(config_file, " not found. Please create this file with the required API configurations.")
                             }
                             
                             # Store the config file path
                             self$config_file <- config_file
                             
                             # Load configuration
                             self$config <- config::get(file = config_file)
                             
                             # Process variable substitutions
                             self$process_substitutions()
                           },
                           
                           # Method to handle variable substitutions like ${study_alias}
                           process_substitutions = function() {
                             # Process mongo collection name if it references study_alias
                             if (!is.null(self$config$mongo) &&
                                 !is.null(self$config$mongo$collection) &&
                                 self$config$mongo$collection == "${study_alias}") {
                               
                               if (!is.null(self$config$study_alias)) {
                                 self$config$mongo$collection <- self$config$study_alias
                               } else {
                                 warning("Cannot substitute ${study_alias} in mongo.collection: study_alias is not defined in config")
                               }
                             }
                             
                             # Add more substitution rules as needed
                           },
                           
                           # Get a specific configuration value
                           get_value = function(path) {
                             # Split the path by dots
                             parts <- strsplit(path, "\\.")[[1]]
                             
                             # Start with the root config
                             result <- self$config
                             
                             # Navigate through the path
                             for (part in parts) {
                               if (is.null(result) || !part %in% names(result)) {
                                 return(NULL)
                               }
                               result <- result[[part]]
                             }
                             
                             return(result)
                           },
                           
                           # Check if a configuration value exists
                           has_value = function(path) {
                             !is.null(self$get_value(path))
                           },
                           
                           # Validate specific API configuration
                           validate_config = function(api_type = NULL) {
                             # If no API type specified, validate core config
                             if (is.null(api_type)) {
                               return(self$validate_core_config())
                             }
                             
                             # Check if the API type is supported
                             if (!api_type %in% names(self$api_specs)) {
                               stop("Unknown API type: '", api_type, "'. Valid options are: ",
                                    paste(names(self$api_specs), collapse=", "))
                             }
                             
                             all_errors <- c()
                             
                             # Check if API section exists
                             if (!self$has_value(api_type)) {
                               stop("The '", api_type, "' section is missing in ", self$config_file,
                                    ". Please add a ", api_type, " section with the necessary configuration.")
                             }
                             
                             # Get API specs
                             specs <- self$api_specs[[api_type]]
                             
                             # Check required fields
                             for (field in specs$required) {
                               field_path <- paste0(api_type, ".", field)
                               if (!self$has_value(field_path)) {
                                 all_errors <- c(all_errors, paste("Missing '", field, "' setting in the ", api_type, " section"))
                               }
                             }
                             
                             # API-specific additional validations
                             if (api_type == "mongo") {
                               # Check if collection is empty after substitution
                               if (self$has_value("mongo.collection") && nchar(self$get_value("mongo.collection")) == 0) {
                                 all_errors <- c(all_errors, "The 'collection' setting cannot be empty")
                               }
                             } else if (api_type == "qualtrics") {
                               # Check if survey_ids exists and is a list
                               if (self$has_value("qualtrics.survey_ids")) {
                                 survey_ids <- self$get_value("qualtrics.survey_ids")
                                 if (!is.list(survey_ids)) {
                                   all_errors <- c(all_errors, "The 'survey_ids' setting must be a nested structure")
                                 } else {
                                   # Check if there are any institutions defined
                                   if (length(names(survey_ids)) == 0) {
                                     all_errors <- c(all_errors, "No institutions defined in 'survey_ids'")
                                   }
                                 }
                               }
                             } else if (api_type == "redcap") {
                               # Any redcap-specific validations
                             } else if (api_type == "sql") {
                               # Any sql-specific validations
                             }
                             
                             # If we found any errors, report them all at once
                             if (length(all_errors) > 0) {
                               stop(api_type, " configuration errors in ", self$config_file, ":\n- ",
                                    paste(all_errors, collapse="\n- "), call. = FALSE)
                             } else {
                               message("The ", api_type, " configuration in ", self$config_file, " is valid.")
                             }
                             
                             return(TRUE)
                           },
                           
                           # Validate core configuration
                           validate_core_config = function() {
                             all_errors <- c()
                             
                             # Check required global fields
                             required_fields <- c("study_alias", "identifier")
                             
                             for (field in required_fields) {
                               if (!self$has_value(field)) {
                                 all_errors <- c(all_errors, paste("Missing required '", field, "' setting in the root configuration"))
                               }
                             }
                             
                             # If we found any errors, report them all at once
                             if (length(all_errors) > 0) {
                               stop("Core configuration errors in ", self$config_file, ":\n- ",
                                    paste(all_errors, collapse="\n- "), call. = FALSE)
                             } else {
                               # message("The core configuration in ", self$config_file, " is valid.")
                             }
                             
                             return(TRUE)
                           }
                         )
)

# Create a function to validate configuration and return the config
validate_config <- function(api_type = NULL, config_file = "config.yml") {
  config_env <- ConfigEnv$new(config_file)
  
  # Validate the configuration
  validation_result <- config_env$validate_config(api_type)
  
  # If validation passes, return the config
  if (validation_result) {
    return(config_env$config)
  } else {
    return(NULL)  # Or handle failure appropriately
  }
}

================
File: R/dataRequest.R
================
#' Data Request
#'
#' This function processes requests for clean data sequentially for specified measures.
#' It makes a request to the appropriate API for the named measure or measures
#' and runs the associated data cleaning routines. It then runs a series of
#' unit tests to verify that the data quality standards are met.
#'
#' @param ... Strings, specifying the measures to process, which can be a Mongo collection, REDCap instrument, or Qualtrics survey.
#' @param csv Optional; Boolean, if TRUE creates a .csv extract in ./tmp.
#' @param rdata Optional; Boolean, if TRUE creates an .rdata extract in ./tmp.
#' @param spss Optional; Boolean, if TRUE creates a .sav extract in ./tmp.
#' @return Prints the time taken for the data request process.
#' @export
#' @examples
#' \dontrun{
#'   dataRequest("prl", csv=TRUE)
#'   dataRequest("rgpts", "kamin", rdata=TRUE)
#' }
#' 
#' @author Joshua Kenney <joshua.kenney@yale.edu>
#' 
dataRequest <- function(..., csv = FALSE, rdata = FALSE, spss = FALSE) {
  
  # Set up cleanup for any MongoDB connections that might persist
  on.exit({
    # Find and cleanup any mongo connections in the global environment
    mongo_objects <- ls(envir = .GlobalEnv, pattern = "^Mongo|_mongo$|^mongo", all.names = TRUE)
    for (obj in mongo_objects) {
      if (exists(obj, envir = .GlobalEnv)) {
        conn <- get(obj, envir = .GlobalEnv)
        if (is.environment(conn) && exists("disconnect", envir = conn)) {
          tryCatch({
            conn$disconnect()
          }, error = function(e) NULL)
        }
      }
    }
    gc()  # Force garbage collection
  })
  
#   base::source("api/testSuite.R")
  
  # Required Libraries Setup
#   if (!require("tidyverse")) {install.packages("tidyverse")}; library(tidyverse)
#   if (!require("dplyr")) {install.packages("dplyr")}; library(dplyr)
#   if (!require("config")) {install.packages("config")}; library(config)
  
  # Prepare lists for REDCap, Qualtrics, and tasks
  redcap_list <- tools::file_path_sans_ext(list.files("./clean/redcap"))
  qualtrics_list <- tools::file_path_sans_ext(list.files("./clean/qualtrics"))
  task_list <- tools::file_path_sans_ext(list.files("./clean/task"))
  
  # Get identifier from config
  config <- config::get()
  identifier <- config$identifier
  if (is.null(identifier) || identifier == "") {
    stop("No identifier specified in the config file.")
  }
  
  # Split identifier if it's a comma-separated string
  if (is.character(identifier)) {
    identifier <- strsplit(identifier, ",")[[1]]
  }
  
  start_time <- Sys.time()
  
  # Source necessary R scripts from the 'api' directory
#   lapply(list.files("api/src", pattern = "\\.R$", full.names = TRUE), base::source)
#   lapply(list.files("api/fn", pattern = "\\.R$", full.names = TRUE), base::source)
  
  # Validate Measures Function
  validateMeasures <- function(data_list) {
    # Check if input is a dataframe
    if (is.data.frame(data_list)) {
      # Get the name of the dataframe as a string
      data_list <- deparse(substitute(data_list))
    }
    
    # Ensure data_list is a character vector (in case it's a single string)
    if (!is.character(data_list)) {
      data_list <- as.character(data_list)
    }
    
    # Validate measures against predefined lists
    invalid_list <- Filter(function(measure) !measure %in% c(redcap_list, qualtrics_list, task_list), data_list)
    
    if (length(invalid_list) > 0) {
      stop(paste(invalid_list, collapse = ", "), " does not have a cleaning script, please create one in clean/.\n")
    }
  }
  
  # Compile data list and validate measures
  data_list <- list(...)
  
  #this is so the function doesn't break if user enters a variable storing a character vector 
  #or a list of strings 
  #in other words it let's you do this:
  #vars_i_want <- c('demo','sps','sips_p')
  #dataRequest(vars_i_want)
  if (length(data_list) == 1){
    data_list = data_list[[1]]
  }
  validateMeasures(data_list)
  
  # Process each measure using processMeasure function
  for (measure in data_list) {
    sourceCategory <- ifelse(measure %in% redcap_list, "redcap", ifelse(measure %in% qualtrics_list, "qualtrics", "task"))
#     base::source("api/dataRequest.R")
    processMeasure(measure, sourceCategory, csv, rdata, spss, identifier)
  }
  
  # Clean up and record processing time
  print(Sys.time() - start_time)  # Print time taken for processing
  
  # Flush environment
#   base::source("api/env/cleanup.R")
  
  return(invisible(NULL))
  
}

processMeasure <- function(measure, source, csv, rdata, spss, identifier) {
  # Check if input is a dataframe
  if (is.data.frame(measure)) {
    # Get the name of the dataframe as a string
    measure <- deparse(substitute(measure))
  }
  
  # Ensure data_list is a character vector (in case it's a single string)
  if (!is.character(measure)) {
    measure <- as.character(measure)
  }
  # Construct the path to the measure's cleaning script
  file_path <- sprintf("./clean/%s/%s.R", source, measure)
  message("\nProcessing ", measure, " from clean/", source, "/", measure)
  
  # Setup cleanup on exit
  on.exit({
    if (exists("mongo_conn") && !is.null(mongo_conn)) {
      tryCatch({
        mongo_conn$disconnect()
      }, error = function(e) {
        warning(sprintf("Error disconnecting from MongoDB: %s", e$message))
      })
    }
    # Clear the mongo connection from memory
    if (exists("mongo_conn")) {
      rm(mongo_conn)
    }
    gc()  # Force garbage collection
  })
  
  result <- tryCatch({
    base::source(file_path)  # Execute the cleaning script for the measure
    # Ensure testSuite is sourced and then called
#     base::source("api/testSuite.R")
    # Call testSuite with identifier
    testSuite(measure, source, file_path, identifier)
    
    df_name <- paste0(measure, "_clean")  # Construct the name of the cleaned data frame
    
    # Assuming createExtract is a function to create data extracts
    createExtract(get(df_name), df_name, csv, rdata, spss)  # Create data extracts
  }, error = function(e) {
    # Check if identifier is valid (you can modify this logic based on your criteria)
    if (length(identifier) == 0 || all(is.na(identifier))) {
      message("An error occurred: ", e$message)  # General error message
    } else {
      message("Error with ", measure, ": ", e$message)  # Specific error message
    }
    NULL  # Return NULL on error
  })
  
  return(result)  # Return the result of the processing
}




# Add helper function for MongoDB cleanup
disconnectMongo <- function(mongo) {
  if (!is.null(mongo)) {
    tryCatch({
      mongo$disconnect()
      rm(list = deparse(substitute(mongo)), envir = parent.frame())
    }, error = function(e) {
      warning(sprintf("Error disconnecting from MongoDB: %s", e$message))
    })
  }
  
}

================
File: R/dataWizardry-package.R
================
#' @keywords internal
"_PACKAGE"

## usethis namespace: start
## usethis namespace: end
NULL

================
File: R/getMongo.R
================
# if (!require(mongolite)) { install.packages("mongolite") }; library(mongolite)
# if (!require(future)) { install.packages("future") }; library(future)
# if (!require(future.apply)) { install.packages("future.apply") }; library(future.apply)
# if (!require(config)) { install.packages("config") }; library(config)
# if (!require(dplyr)) { install.packages("dplyr") }; library(dplyr)

#' Cross-platform memory check function
#' @return List containing total and available memory in GB
#' @noRd
getAvailableMemory <- function() {
      tryCatch({
        if (.Platform$OS.type == "windows") {
          # Windows-specific memory detection with better error handling
          total_mem <- tryCatch({
            mem_info <- system('wmic ComputerSystem get TotalPhysicalMemory /Value', intern = TRUE)
            mem_line <- grep("TotalPhysicalMemory=", mem_info, value = TRUE)
            if (length(mem_line) == 0) return(NULL)
            total <- as.numeric(sub("TotalPhysicalMemory=", "", mem_line))
            if (is.na(total)) return(NULL)
            total / (1024^3)  # Convert to GB
          }, error = function(e) NULL)
          
          avail_mem <- tryCatch({
            # Get multiple memory metrics for better available memory calculation
            mem_info <- system('wmic OS get FreePhysicalMemory,TotalVisibleMemorySize /Value', intern = TRUE)
            
            # Extract free physical memory
            free_line <- grep("FreePhysicalMemory=", mem_info, value = TRUE)
            if (length(free_line) == 0) return(NULL)
            free_mem <- as.numeric(sub("FreePhysicalMemory=", "", free_line))
            if (is.na(free_mem)) return(NULL)
            
            # Get total visible memory for percentage calculation
            total_line <- grep("TotalVisibleMemorySize=", mem_info, value = TRUE)
            if (length(total_line) == 0) return(NULL)
            total_visible <- as.numeric(sub("TotalVisibleMemorySize=", "", total_line))
            if (is.na(total_visible)) return(NULL)
            
            # Convert KB to GB and add 20% buffer for cached memory
            available <- (free_mem / (1024^2)) * 1.2
            
            # Sanity check - don't return more than 90% of total memory
            max_available <- (total_visible / (1024^2)) * 0.9
            min(available, max_available)
          }, error = function(e) NULL)
          
          # Return both metrics, with NULL handling
          return(list(
            total = if (is.null(total_mem)) NULL else round(total_mem, 1),
            available = if (is.null(avail_mem)) NULL else round(avail_mem, 1)
          ))
    } else if (Sys.info()["sysname"] == "Darwin") {
      # MacOS
      total_mem <- tryCatch({
        mem_info <- system("sysctl hw.memsize", intern = TRUE)
        as.numeric(strsplit(mem_info, " ")[[1]][2]) / (1024^3)
      }, error = function(e) NULL)
      
      # More accurate available memory detection for Mac
      avail_mem <- tryCatch({
        vm_stat <- system("vm_stat", intern = TRUE)
        page_size <- 4096  # Default page size for Mac
        
        # Extract different memory stats
        get_pages <- function(pattern) {
          line <- grep(pattern, vm_stat, value = TRUE)
          as.numeric(sub(".*: *(\\d+).*", "\\1", line))
        }
        
        free_pages <- get_pages("Pages free:")
        inactive_pages <- get_pages("Pages inactive:")
        purgeable_pages <- get_pages("Pages purgeable:")
        cached_pages <- get_pages("File-backed pages:")
        
        # Calculate available memory including cache and purgeable
        total_available_pages <- free_pages + inactive_pages + purgeable_pages + cached_pages
        (total_available_pages * page_size) / (1024^3)  # Convert to GB
      }, error = function(e) NULL)
      
      return(list(
        total = total_mem,
        available = avail_mem
      ))
    } else {
      # Linux
      if (file.exists("/proc/meminfo")) {
        mem_info <- readLines("/proc/meminfo")
        
        # Helper function to extract memory values
        get_mem_value <- function(pattern) {
          line <- grep(pattern, mem_info, value = TRUE)
          value <- as.numeric(strsplit(line, "\\s+")[[1]][2])  # Get the number
          value / (1024^2)  # Convert KB to GB
        }
        
        # Get all relevant memory metrics
        total_mem <- get_mem_value("MemTotal:")
        free_mem <- get_mem_value("MemFree:")
        available_mem <- get_mem_value("MemAvailable:")  # Modern Linux kernels provide this
        cached_mem <- get_mem_value("Cached:")
        buffers_mem <- get_mem_value("Buffers:")
        slab_mem <- get_mem_value("SReclaimable:")  # Reclaimable kernel memory
        
        # Calculate true available memory
        # MemAvailable is already calculated by kernel using a sophisticated algorithm
        # But we can fall back to our own calculation if needed
        if (!is.na(available_mem)) {
          avail_mem <- available_mem
        } else {
          # Similar to how the kernel calculates it:
          # free + ((cached + buffers + slab) * 0.8)
          avail_mem <- free_mem + ((cached_mem + buffers_mem + slab_mem) * 0.8)
        }
        
        return(list(
          total = total_mem,
          available = avail_mem
        ))
      }
    }
  }, error = function(e) {
    return(list(total = NULL, available = NULL))
  })
  return(list(total = NULL, available = NULL))
}

#' Calculate optimal resource parameters
#' @param total_records Total number of records to process
#' @param mem_info Memory information structure
#' @param num_cores Number of CPU cores
#' @return List containing optimal chunk size and number of workers
#' @noRd
calculateResourceParams <- function(total_records, mem_info, num_cores) {
  # Default values
  params <- list(
    chunk_size = 1000,
    workers = num_cores  # Use all cores by default
  )
  
  # Adjust chunk size based on available memory
  if (!is.null(mem_info$available)) {
    if (mem_info$available < 4) {
      params$chunk_size <- 500
    } else if (mem_info$available < 8) {
      params$chunk_size <- 1000
    } else if (mem_info$available < 16) {
      params$chunk_size <- 2000
    } else {
      params$chunk_size <- 5000
    }
  }
  
  # Adjust for very small datasets
  if (total_records < params$chunk_size * 2) {
    params$chunk_size <- max(500, floor(total_records / 2))
  }
  
  # Calculate resulting chunks
  params$num_chunks <- ceiling(total_records / params$chunk_size)
  
  return(params)
}

#' Initialize a clean loading animation
#' @param steps Number of steps in the process
#' @return Loading animation object
#' @noRd
initializeLoadingAnimation <- function(steps) {
  list(
    steps = steps,
    current = 0,
    width = 50,
    start_time = Sys.time()
  )
}

#' Update the loading animation
#' @param pb Loading animation object
#' @param current Current step
#' @noRd
updateLoadingAnimation <- function(pb, current) {
  pb$current <- current
  percentage <- round(current / pb$steps * 100)
  filled <- round(pb$width * current / pb$steps)
  bar <- paste0(
    strrep("=", filled),
    strrep(" ", pb$width - filled)
  )
  cat(sprintf("\r  |%s| %3d%%", bar, percentage))
  utils::flush.console()
}

#' Complete the loading animation
#' @param pb Loading animation object
#' @noRd
completeLoadingAnimation <- function(pb) {
  updateLoadingAnimation(pb, pb$steps)
  cat("\n")
}

#' Format a time duration in a human-readable way
#' 
#' @name formatDuration
#' @param duration The duration to format in seconds or minutes
#' @return A formatted string representing the duration
#' @noRd
formatDuration <- function(duration) {
  secs <- as.numeric(duration, units = "secs")
  if (secs < 60) {
    return(sprintf("%.1f seconds", secs))
  } else {
    mins <- floor(secs / 60)
    remaining_secs <- round(secs %% 60, 1)
    if (remaining_secs > 0) {
      return(sprintf("%d minutes and %.1f seconds", mins, remaining_secs))
    } else {
      return(sprintf("%d minutes", mins))
    }
  }
}

#' Retrieve data from MongoDB
#'
#' @param collection_name The name of the MongoDB collection
#' @param db_name The database name (optional)
#' @param identifier Field to use as identifier (optional)
#' @param chunk_size Number of records per chunk (optional)
#'
#' @importFrom mongolite mongo ssl_options
#' @importFrom parallel detectCores
#' @importFrom future plan multisession
#' @importFrom future future
#' @importFrom future.apply future_lapply
#' @importFrom dplyr bind_rows
#' @importFrom utils flush.console
#' @importFrom stats setNames
#'
#' @return A data frame containing the MongoDB data
#' @export
getMongo <- function(collection_name, db_name = NULL, identifier = NULL, chunk_size = NULL) {
  start_time <- Sys.time()
  Mongo <- NULL  # Initialize to NULL for cleanup in on.exit
  
  # Setup cleanup on exit
  on.exit({
    disconnectMongo(Mongo)
  })
  
  # Suppress MongoDB messages globally
  options(mongolite.quiet = TRUE)
  
  # Get configuration
#   base::source("api/ConfigEnv.R")
  cfg <- validate_config("mongo")
  
  if (is.null(db_name)) {
    db_name <- cfg$mongo$collection
  }
  
  # Validate identifier
  if (is.null(identifier)) {
    identifier <- cfg$identifier
  }
  
  if (is.null(identifier) || any(identifier == "")) {
    stop("No identifier specified in the config file.")
  }
  
  # Try connecting - will now throw explicit error if collection doesn't exist
  Mongo <- ConnectMongo(collection_name, db_name)
  
  # Find valid identifier
  if (is.null(identifier)) {
    for (key in trimws(strsplit(identifier, ",")[[1]])) {
      count <- Mongo$count(sprintf('{"%s": {"$exists": true, "$ne": ""}}', key))
      if (count > 0) {
        identifier <- key
        break
      }
    }
  }
  
  if (is.null(identifier)) {
    stop("No valid identifier found in the collection.")
  }
  
  # message(sprintf("Using identifier: %s", identifier))
  
  # Get total records
  query_json <- sprintf('{"%s": {"$ne": ""}}', identifier)
  total_records <- Mongo$count(query_json)
  
  # Get and display system resources
  mem_info <- getAvailableMemory()
  num_cores <- parallel::detectCores(logical = TRUE)
  workers <- num_cores
  
  # Display system info
  if (!is.null(mem_info$total)) {
    message(sprintf("System resources: %.0fGB RAM, %d-core CPU", 
                    mem_info$total, num_cores))
  } else {
    message(sprintf("System resources: %d-core CPU.", num_cores))
  }
  
  # Calculate parameters once
  params <- calculateResourceParams(total_records, mem_info, num_cores)
  
  if (!is.null(mem_info$available)) {
    message(sprintf("Memory available: %.0fGB RAM", mem_info$available))
  }
  
  # Adjust chunk size based on memory
  if (is.null(chunk_size)) {  # Only if not manually specified
    if (!is.null(mem_info$available)) {
      if (mem_info$available < 4) {
        chunk_size <- 500
      } else if (mem_info$available < 8) {
        chunk_size <- 1000
      } else if (mem_info$available < 16) {
        chunk_size <- 2000
      } else {
        chunk_size <- 5000
      }
    } else {
      chunk_size <- 1000  # Conservative default
    }
  }
  
message(sprintf("Processing: %d chunks x %d records in parallel (%d workers)", 
                params$num_chunks, params$chunk_size, params$workers))
  
  # Setup chunks
  num_chunks <- ceiling(total_records / chunk_size)
  chunks <- createChunks(total_records, chunk_size)
  
  # Setup parallel processing with quiet connections
  plan(future::multisession, workers = workers)
  

  
  # Progress message
  #message("Retrieving data:")
  #message(sprintf("Found %d records in %s/%s", total_records, db_name, collection_name))
  message(sprintf("\nImporting %s records from %s/%s into dataframe...", 
                  formatC(total_records, format = "d", big.mark = ","), 
                  db_name, collection_name))

  # Initialize custom progress bar
  pb <- initializeLoadingAnimation(num_chunks)

  
  # Process chunks
  future_results <- vector("list", length(chunks))
  for (i in seq_along(chunks)) {
    future_results[[i]] <- future({
      temp <- tempfile()
      sink(temp)
      chunk_mongo <- NULL  # Initialize connection variable
      
      on.exit({
        sink()
        unlink(temp)
        disconnectMongo(chunk_mongo)  # Cleanup connection in worker
      })
      
      tryCatch({
        chunk_mongo <- ConnectMongo(collection_name, db_name)
        batch_info <- chunks[[i]]
        if (!is.null(batch_info) && !is.null(batch_info$start) && !is.null(batch_info$size)) {
          data_chunk <- getMongoData(chunk_mongo, identifier, batch_info)
        } else {
          warning("Invalid batch info, skipping chunk")
          return(NULL)
        }
        data_chunk
      }, error = function(e) {
        warning(sprintf("Error processing chunk %d: %s", i, e$message))
        NULL
      })
    })
    updateLoadingAnimation(pb, i)
  }
  
  # Collect results
  results <- lapply(future_results, future::value)
  
  # Combine results
  # message("\nCombining data chunks...")
  df <- dplyr::bind_rows(results)
  completeLoadingAnimation(pb)
  
  # Harmonize data
  message(sprintf("Harmonizing data on %s...", identifier), appendLF = FALSE)  # Prevents line feed
  clean_df <- taskHarmonization(df, identifier, collection_name)
  Sys.sleep(0.5)  # Optional: small pause for visual effect
  message(sprintf("\rHarmonizing data on %s...done.", identifier))  # Overwrites the line with 'done'
  # "\u2713"
  
  # Report execution time
  end_time <- Sys.time()
  duration <- difftime(end_time, start_time, units = "secs")
  Sys.sleep(0.5)  # Optional: small pause for visual effect
  message(sprintf("\nData frame '%s' retrieved in %s.", collection_name, formatDuration(duration-1)))# minus 1 to account for sleep
  
  return(clean_df)
}

# ################ #
# Helper Functions #
# ################ #

createChunks <- function(total_records, chunk_size) {
  tryCatch({
    num_chunks <- ceiling(total_records / chunk_size)
    chunks <- vector("list", num_chunks)
    for (i in seq_len(num_chunks)) {
      chunks[[i]] <- list(
        start = (i - 1) * chunk_size,
        size = if (i == num_chunks) {
          min(chunk_size, total_records - ((i - 1) * chunk_size))
        } else {
          chunk_size
        }
      )
    }
    return(chunks)
  }, error = function(e) {
    warning("Error creating chunks, falling back to single chunk")
    return(list(list(start = 0, size = total_records)))
  })
}

#' Setup MongoDB connection with suppressed messages
#' @param collection_name The name of the collection you want to connect to.
#' @param db_name The name of the database you cant to connect to.
#' @return A mongolite::mongo object representing the connection to the MongoDB collection.
#' @noRd
ConnectMongo <- function(collection_name, db_name) {
  # Validate secrets
#   base::source("api/SecretsEnv.R")
  validate_secrets("mongo")
  
#   base::source("api/ConfigEnv.R")
  config <- validate_config("mongo")
  
  if (is.null(db_name)) {
    db_name = config$mongo$collection
  }
  options <- ssl_options(weak_cert_validation = TRUE, key = "rds-combined-ca-bundle.pem")
  
  # The key is to use sink() to capture and discard the messages
  temp <- tempfile()
  sink(temp)
  
  # Create connection without specifying collection first
  base_connection <- mongolite::mongo(
    collection = "system.namespaces", # This is a system collection that always exists
    db = db_name,
    url = connectionString,
    verbose = FALSE,
    options = options
  )
  
  # Check if the collection exists
  collections_list <- getCollectionsFromConnection(base_connection)
  
  # Close the base connection
  base_connection$disconnect()
  sink()
  unlink(temp)
  
  # Validate that collection exists
  if (!collection_name %in% collections_list) {
    stop(sprintf("Collection '%s' does not exist in database '%s'. Available collections: %s", 
                 collection_name, db_name, paste(collections_list, collapse=", ")))
  }
  
  # If we get here, the collection exists - create normal connection
  sink(temp)
  on.exit({
    sink()
    unlink(temp)
  })
  
  Mongo <- mongolite::mongo(
    collection = collection_name, 
    db = db_name,
    url = connectionString,
    verbose = FALSE,
    options = options
  )
  
  return(Mongo)
}

#' Safely close MongoDB connection
#' @param mongo A mongolite::mongo connection object
#' @noRd
disconnectMongo <- function(mongo) {
  if (!is.null(mongo)) {
    tryCatch({
      mongo$disconnect()
    }, error = function(e) {
      warning(sprintf("Error disconnecting from MongoDB: %s", e$message))
    })
  }
}

#' Retrieve Task Data
#'
#' Retrieves data from MongoDB based on the specified batch information and query criteria. 
#' It filters out entries where the specified identifier doesn't exist or is empty.
#'
#' @param Mongo The MongoDB connection object.
#' @param identifier The document field to check for existence and non-emptiness.
#' @param batch_info List containing 'start' and 'size' defining the batch to fetch.
#' @return A data.frame with the filtered data or NULL if no valid data is found or in case of error.
#' @examples
#' # This example assumes 'Mongo' is a MongoDB connection
#' # batch_info <- list(start = 0, size = 100)
#' # df <- getMongoData(Mongo, "src_subject_id", batch_info)
#' @noRd
getMongoData <- function(Mongo, identifier, batch_info) {
  # Check for both exists AND non-empty
  query_json <- sprintf('{"%s": {"$exists": true, "$ne": ""}}', identifier)
  print(paste("Using query:", query_json))
  
  # Get initial data
  df <- Mongo$find(query = query_json, skip = batch_info$start, limit = batch_info$size)
  print(paste("Initial rows:", nrow(df)))
  
  # Only proceed with filtering if we have data
  if (!is.null(df) && nrow(df) > 0) {
    # Print sample of data before filtering
    print("Sample before filtering:")
    print(head(df[[identifier]]))
    
    # Apply both NA and empty string filtering
    df <- df[!is.na(df[[identifier]]) & df[[identifier]] != "", ]
    print(paste("Rows after complete filtering:", nrow(df)))
    
    # Print sample after filtering
    print("Sample after filtering:")
    print(head(df[[identifier]]))
  } else {
    print("No data found in initial query")
  }
  
  return(df)
}


#' Task Data Harmonization Function
#'
#' This function performs data cleaning and preparation tasks, including handling missing values, 
#' converting date formats, and adding necessary columns. It is tailored for a specific dataset 
#' structure used in psychological or medical research.
#'
#' @param df A data frame containing the data to be harmonized. 
#' @param identifier A string that specifies the unique identifier for the dataset; 
#' it influences how date conversions and subsetting are handled.
#' @param collection_name A string representing the specific collection that needs harmonization.
#'
#' @return A data frame with the harmonized data, including standardized 'visit' column entries, 
#' converted interview dates, and added 'measure' column based on the task.
#'
#' @examples
#' \dontrun{
#' # Create a sample dataset
#' df <- data.frame(
#'   src_subject_id = 1:3,
#'   visit = c("bl", "6m", "12m"),
#'   score = c(10, 20, 30)
#' )
#' harmonized_data <- taskHarmonization(df, 'src_subject_id', 'task1')
#' }
#'
#' @importFrom stats setNames
#' @noRd
taskHarmonization <- function(df, identifier, collection_name) {
  
  # Ensure 'visit' column exists and update it as necessary
  if (!("visit" %in% colnames(df))) {
    df$visit <- "bl"  # Add 'visit' column with all values as "bl" if it doesn't exist
  } else {
    df$visit <- ifelse(is.na(df$visit) | df$visit == "", "bl", df$visit)  # Replace empty or NA 'visit' values with "bl"
  }
  
  # capr wants as.numeric
  # if (config$mongo$collection === "capr") {
  #   df$src_subject_id <- as.numeric(df$src_subject_id)
  # }
  
  # convert dates (from string ("m/d/Y") to date format)
  interview_date_exists <- "interview_date" %in% colnames(df)
  
  if (interview_date_exists) {
    df$interview_date <- as.Date(df$interview_date, "%m/%d/%Y")
  }
  
  # add measure column
  # df$measure <- collection_name
  
  return(df)
  # comment into add prefixes (will break code)
  #return(add_prefix_to_columns(df,collection_name))
  
}

getCollectionsFromConnection <- function(mongo_connection) {
  collections <- mongo_connection$run('{"listCollections":1,"nameOnly":true}')
  return(collections$cursor$firstBatch$name)
}

# Maintain original getCollections function for backward compatibility
getCollections <- function() {
  Mongo <- NULL
  on.exit({
    disconnectMongo(Mongo)
  })
  
  # Connect to any default collection just to get connection
  # Mongo <- ConnectMongo("system.namespaces", silent_validation = TRUE)
  Mongo <- ConnectMongo("system.namespaces")
  collections <- getCollectionsFromConnection(Mongo)
  return(collections)
}


#' Alias for 'getMongo'
#'
#' This is a legacy alias for the 'getMongo' function to maintain compatibility with older code.
#'
#' @inheritParams getMongo
#' @inherit getMongo return
#' @export
#' @examples
#' \dontrun{
#' survey_data <- getTask("task_alias")
#' }
getTask <- getMongo

================
File: R/getQualtrics.R
================
#' Retrieve Survey Data from Qualtrics
#'
#' @param qualtrics_alias The alias for the Qualtrics survey to be retrieved.
#' @param institution Optional. The institution name (e.g., "temple" or "nu"). If NULL, all institutions will be searched.
#' @param label Logical indicating whether to return coded values or their associated labels (default is FALSE).
#' @return A cleaned and harmonized data frame containing the survey data.
#' @importFrom dplyr %>% select mutate
#' @export
#' @examples
#' \dontrun{
#' # Get survey by alias (will search all institutions)
#' survey_data <- getQualtrics("rgpts")
#' }
getQualtrics <- function(qualtrics_alias, institution = NULL, label = FALSE) {
  
#   lapply(list.files("api/src", pattern = "\\.R$", full.names = TRUE), base::source)
  
  # Get configuration
  cfg <- config::get()
  
  # Get survey ID
  survey_id <- NULL
  
  if (!is.null(institution)) {
    # Check if institution exists
    if (!(institution %in% names(cfg$qualtrics$survey_ids))) {
      stop(paste("Institution", institution, "not found in configuration"))
    }
    
    # Check if survey exists in specified institution
    if (!(qualtrics_alias %in% names(cfg$qualtrics$survey_ids[[institution]]))) {
      stop(paste("Survey", qualtrics_alias, "not found in institution", institution))
    }
    
    survey_id <- cfg$qualtrics$survey_ids[[institution]][[qualtrics_alias]]
  } else {
    # Search all institutions
    for (inst in names(cfg$qualtrics$survey_ids)) {
      if (qualtrics_alias %in% names(cfg$qualtrics$survey_ids[[inst]])) {
        survey_id <- cfg$qualtrics$survey_ids[[inst]][[qualtrics_alias]]
        institution <- inst
        break
      }
    }
  }
  
  if (is.null(survey_id)) {
    stop(paste("Survey", qualtrics_alias, "not found in any institution"))
  }
  
  message(sprintf("Retrieving %s survey from %s", qualtrics_alias, institution))
  
  # Connect to Qualtrics
  connectQualtrics(qualtrics_alias)
  
  # Show loading animation (if implemented)
  if (exists("show_loading_animation")) {
    show_loading_animation()
  }
  
  # Fetch the data
  df <- qualtRics::fetch_survey(
    surveyID = survey_id,
    verbose = FALSE,
    label = label,
    convert = label,
    add_column_map = TRUE
  )
  
  if (!is.data.frame(df)) {
    stop(paste("fetch_survey did not return a data frame for", qualtrics_alias))
  }
  
  # Get identifier from config
  identifier <- cfg$identifier
  
  # Harmonize the data
  clean_df <- qualtricsHarmonization(df, identifier, qualtrics_alias)
  
  return(clean_df)
}

# ################ #
# Helper Functions #
# ################ #

#' Connect to Qualtrics API
#'
#' This helper function sets up the connection to the Qualtrics API using credentials stored in a file or environment variables.
#' It is called internally by the 'getSurvey' function.
#'
#' @param qualtrics_alias The alias for the Qualtrics survey to connect to.
#' @importFrom config get
#' @import qualtRics
#' @noRd
connectQualtrics <- function(qualtrics_alias) {
  
  # Validate secrets
#   base::source("api/SecretsEnv.R")
  validate_secrets("qualtrics")
  
  # Validate config
#   base::source("api/ConfigEnv.R")
  validate_config("qualtrics")
  
  #base::source(config$qualtrics$survey_ids)
  # NEW CODE
  cfg <- config::get()
  for (inst in names(cfg$qualtrics$survey_ids)) {
    if (qualtrics_alias %in% names(cfg$qualtrics$survey_ids[[inst]])) {
      survey_id <- cfg$qualtrics$survey_ids[[inst]][[qualtrics_alias]]
      break
    }
  }
  
  #if (!(qualtrics_alias %in% names(surveyIds))) {
  #  stop("Provided qualtrics_alias does not match any survey IDs.")
  #}
  
  if (!exists("apiKeys") || !exists("baseUrls")) {
    stop("apiKeys and/or baseUrls arrays not found in secrets.R")
  }
  if (length(apiKeys) != length(baseUrls)) {
    stop("apiKeys and baseUrls arrays must have the same length")
  }
  
  for (i in seq_along(apiKeys)) {
    tryCatch({
      qualtRics::qualtrics_api_credentials(
        api_key = apiKeys[i],
        base_url = baseUrls[i],
        install = TRUE,
        overwrite = TRUE
      )
      return(TRUE)
    }, error = function(e) {
      if (i == length(apiKeys)) {
        stop("Failed to connect with any credentials")
      }
    })
  }
}

#' Retrieve Data from Qualtrics
#'
#' Fetches survey data from Qualtrics based on the survey alias and label preference. 
#' It attempts to fetch survey data and handle any errors that occur.
#'
#' @param qualtrics_alias The alias for the Qualtrics survey whose data is to be fetched.
#' @param label Logical indicating whether to fetch choice labels instead of coded values.
#' @return Data frame containing survey data, or NULL in case of error.
#' @importFrom qualtRics fetch_survey
#' @noRd
getQualtricsData <- function(qualtrics_alias, label) {
  tryCatch({
    
    # Validate config
#     base::source("api/ConfigEnv.R")
    validate_config("qualtrics")
    
    df <- qualtRics::fetch_survey(
      surveyID = toString(surveyIds[qualtrics_alias]),
      verbose = FALSE,
      label = label,
      convert = label,
      add_column_map = TRUE
    )
    if (!is.data.frame(df)) {
      stop(paste("fetch_survey did not return a data frame for", qualtrics_alias))
    }
    return(df)
  }, error = function(e) {
    message("Error in getQualtricsData: ", e$message)
    return(NULL)
  })
}

#' Harmonize Data
#'
#' Performs data cleaning and harmonization on the fetched Qualtrics survey data.
#'
#' @param df Data frame containing Qualtrics survey data.
#' @param identifier The unique identifier for survey respondents.
#' @param qualtrics_alias The alias for the Qualtrics survey.
#' @return Harmonized data frame.
#' @importFrom dplyr mutate
#' @noRd
qualtricsHarmonization <- function(df, identifier, qualtrics_alias) {
  if (!is.data.frame(df)) {
    stop("Input to qualtricsHarmonization is not a data frame")
  }
  
  # check for visit variable, if not add baseline
  if ("visit" %!in% colnames(df)) {
    df$visit <- "bl"
  }
  
  # if visit variable exists, make sure they are named according to convention
  if ("visit" %in% colnames(df)) {
    df$visit <- ifelse(is.na(df$visit), "bl", ifelse(df$visit == "0", "bl", ifelse(df$visit == "12", "12m", ifelse(df$visit == "24", "24m", df$visit))))
  }
  
  # df$src_subject_id <- as.numeric(df$src_subject_id)
  
  # convert dates
  # df$interview_date <- as.Date(df$interview_date, "%m/%d/%Y")
  
  # add measure column
  # df$measure <- qualtrics_alias
  
  # select visit
  # df <- df[df$visit==visit,]
  
  suppressWarnings(return(df))
  # comment into add prefixes (will break code)
  #suppressWarnings(return(add_prefix_to_columns(df,qualtrics_alias)))
}

#' Extract Column Mapping from Qualtrics Data Frame
#'
#' This function extracts column mappings from the metadata of a Qualtrics survey data frame.
#'
#' @param qualtrics_df Data frame obtained from Qualtrics.
#' @return A list containing the mappings of column names to survey questions.
#' @noRd
getDictionary <- function(qualtrics_df) {
  return(qualtRics::extract_colmap(respdata = qualtrics_df))
}

#' Alias for 'getQualtrics'
#'
#' This is a legacy alias for the 'getQualtrics' function to maintain compatibility with older code.
#'
#' @inheritParams getQualtrics
#' @inherit getQualtrics return
#' @export
#' @examples
#' \dontrun{
#' survey_data <- getSurvey("your_survey_alias")
#' }
getSurvey <- getQualtrics

================
File: R/getRedcap.R
================
#
# function: getRedcap(instrument_name)
# input: instrument_name from table below
#

# Get full file paths of all R files in the api directory
# base::source all files using lapply()
# lapply(list.files("api/src", pattern = "\\.R$", full.names = TRUE), base::source)

# Initialize functions needed for the progress bar
#' @noRd
initializeLoadingAnimation <- function(steps) {
  # Get console width
  width <- tryCatch({
    if (requireNamespace("cli", quietly = TRUE)) {
      cli::console_width() - 10  # Leave some margin
    } else {
      getOption("width", 80) - 10  # Fallback to R's width setting
    }
  }, error = function(e) 80)  # Default if all else fails
  
  list(
    steps = steps,
    current = 0,
    width = width,
    start_time = Sys.time()
  )
}

#' @noRd
updateLoadingAnimation <- function(pb, current) {
  pb$current <- current
  percentage <- round(current / pb$steps * 100)
  filled <- round(pb$width * current / pb$steps)
  bar <- paste0(
    strrep("=", filled),
    strrep(" ", pb$width - filled)
  )
  cat(sprintf("\r|%s| %3d%%", bar, percentage))  # Removed extra spaces before bar
  utils::flush.console()
}

#' @noRd
completeLoadingAnimation <- function(pb) {
  updateLoadingAnimation(pb, pb$steps)
  cat("\n")
}

#' Format a time duration in a human-readable way
#' 
#' @name formatDuration
#' @param duration The duration to format in seconds or minutes
#' @return A formatted string representing the duration
#' @noRd 
formatDuration <- function(duration) {
  secs <- as.numeric(duration, units = "secs")
  if (secs < 60) {
    return(sprintf("%.1f seconds", secs))
  } else {
    mins <- floor(secs / 60)
    remaining_secs <- round(secs %% 60, 1)
    if (remaining_secs > 0) {
      return(sprintf("%d minutes and %.1f seconds", mins, remaining_secs))
    } else {
      return(sprintf("%d minutes", mins))
    }
  }
}

#' Get Data from REDCap
#'
#' Retrieves data from a REDCap instrument
#'
#' @param instrument_name Name of the REDCap instrument
#' @param raw_or_label Whether to return raw or labeled values
#' @param redcap_event_name Optional event name filter
#' @param batch_size Number of records to retrieve per batch
#' @param records Optional vector of specific record IDs
#' @param fields Optional vector of specific fields
#'
#' @importFrom REDCapR redcap_read redcap_instruments redcap_metadata_read
#' @importFrom cli console_width
#' @importFrom knitr kable
#'
#' @return A data frame containing the requested REDCap data
#' @export
getRedcap <- function(instrument_name = NULL, raw_or_label = "raw", 
                      redcap_event_name = NULL, batch_size = 1000, 
                      records = NULL, fields = NULL) {
  start_time <- Sys.time()
  
#   if (!require(config)) install.packages("config"); library(config)
#   if (!require(REDCapR)) install.packages("REDCapR"); library(REDCapR)
#   if (!require(tidyverse)) install.packages("tidyverse"); library(tidyverse)
  
  # Validate secrets
#   base::source("api/SecretsEnv.R")
  validate_secrets("redcap")
  
  # Input validation and config setup
  if (is.null(instrument_name)) {
    forms_data <- REDCapR::redcap_instruments(redcap_uri = uri, token = token, verbose = FALSE)$data
    forms_filtered <- forms_data[!grepl("nda", forms_data$instrument_name), ]
    random_instrument <- sample(forms_filtered$instrument_name, 1)
    forms_table <- paste(capture.output(print(getForms())), collapse = "\n")
    example_text <- sprintf("\n\nExample:\n%s <- getRedcap(\"%s\")", random_instrument, random_instrument)
    stop(sprintf("No REDCap Instrument Name provided!\n%s%s",
                 forms_table, example_text),
         call. = FALSE)
  }
  
  # Validate config
#   base::source("api/ConfigEnv.R")
  config <- validate_config("redcap")

  # Progress bar
  pb <- initializeLoadingAnimation(20)
  message(sprintf("\nImporting records from REDCap form: %s", instrument_name))
  for (i in 1:20) {
    updateLoadingAnimation(pb, i)
    Sys.sleep(0.1)
  }
  completeLoadingAnimation(pb)
  message("")
  
  # First try the simple approach
  tryCatch({
    df <- REDCapR::redcap_read(
      redcap_uri = uri,
      token = token,
      forms = c(config$redcap$super_keys, instrument_name),
      batch_size = batch_size,
      records = records,
      fields = fields,
      raw_or_label = raw_or_label,
      raw_or_label_headers = "raw",
      verbose = TRUE
    )$data
    
    # Quick validation check - if we're missing key data, throw an error to trigger fallback
    required_cols <- c("src_subject_id", "subjectkey")  # Add other required columns as needed
    if (!all(required_cols %in% names(df))) {
      stop("Missing required columns in simple merge")
    }
  }, error = function(e) {
    # If simple approach fails, try the separate keys approach
    message("\nAttempting alternative data retrieval method...")
    
    # Get super_keys data
    super_keys_data <- REDCapR::redcap_read(
      redcap_uri = uri,
      token = token,
      forms = config$redcap$super_keys,
      batch_size = batch_size,
      records = records,
      raw_or_label = raw_or_label,
      raw_or_label_headers = "raw",
      verbose = TRUE
    )$data
    
    # Get instrument data
    instrument_data <- REDCapR::redcap_read(
      redcap_uri = uri,
      token = token,
      forms = instrument_name,
      batch_size = batch_size,
      records = records,
      fields = fields,
      raw_or_label = raw_or_label,
      raw_or_label_headers = "raw",
      verbose = TRUE
    )$data
    
    # Get join keys while preserving redcap_event_name
    join_keys <- base::intersect(names(super_keys_data), names(instrument_data))
    join_keys <- join_keys[join_keys != "redcap_event_name"]
    
    # Merge while preserving the instrument data's redcap_event_name
    df <- base::merge(super_keys_data, instrument_data, by = join_keys, all.y = TRUE)
  })
  
  # Add measure column
  # df$measure <- instrument_name
  
  # For interview_age columns
  age_cols <- grep("_interview_age$", base::names(df))
  if (length(age_cols) > 0) {
    base::names(df)[age_cols] <- "interview_age"
  }
  
  # For interview_date columns
  date_cols <- grep("_interview_date$", base::names(df))
  if (length(date_cols) > 0) {
    base::names(df)[date_cols] <- "interview_date"
  }
  
  # Apply redcap_event_name filter if specified
  if (!is.null(redcap_event_name)) {
    if (!"redcap_event_name" %in% names(df)) {
      stop("Cannot filter by redcap_event_name: column not found in data")
    }
    df <- df[df$redcap_event_name == redcap_event_name, ]
  }
  
  # Study-specific processing
  if (config$study_alias == "impact-mh") {
    if ("dob" %in% colnames(df)) {
      df <- subset(df, select = -dob)
    }
  }
  
  if (config$study_alias == "capr") {
#     base::source("api/redcap/capr-logic.R")
    df <- processCaprData(df, instrument_name)
  }
  
  # Show duration
  end_time <- Sys.time()
  duration <- difftime(end_time, start_time, units = "secs")
  message(sprintf("\nData frame '%s' retrieved in %s.", instrument_name, formatDuration(duration)))

  return(df)
  # comment into add prefixes (will break code)
  #return(add_prefix_to_columns(df,instrument_name))
}



getForms <- function() {
#   if (!require(REDCapR)) install.packages("REDCapR"); library(REDCapR)
#   if (!require(knitr)) install.packages("knitr"); library(knitr)

  # Validate secrets
#   base::source("api/SecretsEnv.R")
  validate_secrets("redcap")
  
  forms <- REDCapR::redcap_instruments(redcap_uri = uri, token = token, verbose = FALSE)$data
  
  # Option 1: Using knitr::kable for a clean table
  return(knitr::kable(forms, format = "simple"))
}

getDictionary <- function(instrument_name) {
#   if (!require(REDCapR)) install.packages("REDCapR"); library(REDCapR)

  # Validate secrets
#   base::source("api/SecretsEnv.R")
  validate_secrets("redcap")
  
  metadata <- REDCapR::redcap_metadata_read(redcap_uri = uri, token = token, verbose = TRUE, config_options = NULL)$data
  dictionary <- metadata[metadata$form_name == instrument_name, ]
  # View(dictionary)
  return(dictionary)
}

================
File: R/globals.R
================
# globals.R
utils::globalVariables(c(
  "connectionString", "apiKeys", "baseUrls", "config",
  "uri", "token", "dob", "surveyIds", "pb", "value"
))

================
File: R/ndaCheckQualtricsDuplicates.R
================
#' Check for Duplicates in Qualtrics Data
#'
#' This function checks for duplicate records in a specified Qualtrics dataset
#' based on certain identifiers and time points. If duplicates are found,
#' they are exported to a CSV file for review and additionally displayed in the viewer.
#'
#' @param measure_alias A string representing the name of the dataset to check for duplicates.
#' @param measure_type A string specifying the type of measure, currently supports 'qualtrics' only.
#' @return The function does not return a value but will generate a CSV file if duplicates are found
#'         and display those duplicates in the RStudio viewer.
#' @export
#' @examples
#' checkDuplicates("your_dataset_alias", "qualtrics")
#' @importFrom dplyr filter %>% 
#' @importFrom testthat test_that expect_true
#' @note This function requires the dplyr and testthat packages. It is specifically designed for
#'       Qualtrics data and expects the data frame to be named with a '_clean' suffix.
#'       It checks for duplicates based on 'src_subject_id' combined with 'visit' or 'week' columns.
#'       The function will stop and throw an error if the necessary columns are not present.
ndaCheckQualtricsDuplicates <- function(measure_alias, measure_type) {
  
  # Ensure required packages are loaded
#   if (!require(dplyr)) { install.packages("dplyr") }; library(dplyr)
#   if (!require(testthat)) { install.packages("testthat") }; library(testthat)
  
  identifier <- "src_subject_id"
  
  df <- base::get0(measure_alias)
  
  if (measure_type == "qualtrics") {
    if (!(identifier %in% colnames(df))) {
      stop("Please provide a valid identifier: src_subject_id, workerId, PROLIFIC_PID")
    }
    
    for (col in c("visit", "week")) {
      if (col %in% base::colnames(df)) {
        df$duplicates <- base::duplicated(df[c(identifier, col)], first = TRUE)
        df_dup_ids <- base::subset(df, duplicates == TRUE)[, c(identifier, col)]
        
        if (base::nrow(df_dup_ids) > 0) {
          # Using inner_join to filter duplicates correctly
          df_duplicates <- df %>% 
            dplyr::inner_join(df_dup_ids, by = c(identifier, col))
          
          if (base::nrow(df_duplicates) > 0) {
            # Export and create a CSV file if duplicates found
            duplicate_extract <- paste0("duplicates_", measure_alias)
            createCsv(df_duplicates, paste0("duplicates_", measure_alias))
            
            tryCatch({
              testthat::test_that("Check for Qualtrics duplicates", {
                testthat::expect_true(base::nrow(df_duplicates) == 0, 
                                      info = paste("DATA ERROR: Duplicates detected in '", measure_alias, "': ", 
                                                   "Offending IDs: ", toString(base::unique(df_duplicates[[identifier]]))))
              })
            }, error = function(e) {
              duplicates_summary <- toString(base::unique(df_duplicates[[identifier]]))
              message(paste("Error in testing for duplicates in '", measure_alias, "': ", e$message, 
                            "\nOffending IDs: ", duplicates_summary,
                            "\nDetails exported to ", paste0("./tmp/",duplicate_extract,".csv")))
            })
            
            # Optionally view the offending records in RStudio's data viewer
            View(df_duplicates)
          }
        }
      }
    }
  }
}

================
File: R/ndaRequest.R
================
#' NDA Request
#'
#' This function processes requests for clean data sequentially for specified measures.
#' It makes a request to the NIH NDA API for the named data structures
#' and runs the associated data remediation routines. It then runs a series of
#' unit tests to verify that the data quality standards are met.
#'
#' @param ... Strings, specifying the measures to process, which can be a Mongo collection, REDCap instrument, or Qualtrics survey.
#' @param csv Optional; Boolean, if TRUE creates a .csv extract in ./tmp.
#' @param rdata Optional; Boolean, if TRUE creates an .rdata extract in ./tmp.
#' @param spss Optional; Boolean, if TRUE creates a .sav extract in ./tmp.
#' @param limited_dataset Optional; Boolean, if TRUE does not perform date-shifting of interview_date or age-capping of interview_age
#' @return Prints the time taken for the data request process.
#' @export
#' @examples
#' \dontrun{
#'   ndaRequest("prl", csv=TRUE)
#'   ndaRequest("rgpts", "kamin", rdata=TRUE)
#' }
#' 
#' @author Joshua Kenney <joshua.kenney@yale.edu>
#' 
ndaRequest <- function(..., csv = FALSE, rdata = FALSE, spss = FALSE, limited_dataset = FALSE) {
  
  start_time <- Sys.time()
  
#   base::source("api/getRedcap.R")
#   base::source("api/getSurvey.R")
#   base::source("api/getTask.R")
  
  # Set up cleanup for any MongoDB connections that might persist
  on.exit({
    # Find and cleanup any mongo connections in the global environment
    mongo_objects <- ls(envir = .GlobalEnv, pattern = "^Mongo|_mongo$|^mongo", all.names = TRUE)
    for (obj in mongo_objects) {
      if (exists(obj, envir = .GlobalEnv)) {
        conn <- get(obj, envir = .GlobalEnv)
        if (is.environment(conn) && exists("disconnect", envir = conn)) {
          tryCatch({
            conn$disconnect()
          }, error = function(e) NULL)
        }
      }
    }
    gc()  # Force garbage collection
  })
  
#   base::source("api/ndaValidator.R")
  
  # Required Libraries Setup
#   if (!require("tidyverse")) {install.packages("tidyverse")}; library(tidyverse)
#   if (!require("dplyr")) {install.packages("dplyr")}; library(dplyr)
#   if (!require("config")) {install.packages("config")}; library(config)
#   if (!require("beepr")) {install.packages("beepr")}; library(beepr)
  
  # Prepare lists for REDCap, Qualtrics, and tasks
  redcap_list <- tools::file_path_sans_ext(list.files("./nda/redcap"))
  qualtrics_list <- tools::file_path_sans_ext(list.files("./nda/qualtrics"))
  task_list <- tools::file_path_sans_ext(list.files("./nda/mongo"))
  
  # Get identifier from config
  config <- config::get()
  identifier <- config$identifier
  if (is.null(identifier) || identifier == "") {
    stop("No identifier specified in the config file.")
  }
  
  # Split identifier if it's a comma-separated string
  if (is.character(identifier)) {
    identifier <- strsplit(identifier, ",")[[1]]
  }
  
  # Source necessary R scripts from the 'api' directory
#   lapply(list.files("api/src", pattern = "\\.R$", full.names = TRUE), base::source)
#   lapply(list.files("api/fn", pattern = "\\.R$", full.names = TRUE), base::source)
  
  # Validate Measures Function
  validateMeasures <- function(data_list) {
    # Check if input is a dataframe
    if (is.data.frame(data_list)) {
      # Get the name of the dataframe as a string
      data_list <- deparse(substitute(data_list))
    }
    
    # Ensure data_list is a character vector (in case it's a single string)
    if (!is.character(data_list)) {
      data_list <- as.character(data_list)
    }
    
    # Validate measures against predefined lists
    invalid_list <- Filter(function(measure) !measure %in% c(redcap_list, qualtrics_list, task_list), data_list)
    
    if (length(invalid_list) > 0) {
      stop(paste(invalid_list, collapse = ", "), " does not have a cleaning script, please create one in nda/.\n")
    }
  }
  
  # Compile data list and validate measures
  data_list <- list(...)
  
  #this is so the function doesn't break if user enters a variable storing a character vector 
  #or a list of strings 
  #in other words it let's you do this:
  #vars_i_want <- c('demo','sps','sips_p')
  #dataRequest(vars_i_want)
  if (length(data_list) == 1){
    data_list = data_list[[1]]
  }
  validateMeasures(data_list)
  
  # Process each measure using processMeasure function
  for (measure in data_list) {
    api <- ifelse(measure %in% redcap_list, "redcap", ifelse(measure %in% qualtrics_list, "qualtrics", "mongo"))
    processMeasure(measure, api, csv, rdata, spss, identifier, start_time, limited_dataset)
  }
  
  # Clean up and record processing time
  # performCleanup()
  # print(Sys.time() - start_time)  # Print time taken for processing
}

processMeasure <- function(measure, api, csv, rdata, spss, identifier, start_time, limited_dataset = FALSE) {
  # Check if input is a dataframe
  if (is.data.frame(measure)) {
    # Get the name of the dataframe as a string
    measure_name <- deparse(substitute(measure))
  } else {
    measure_name <- measure
  }
  
  # Ensure data_list is a character vector (in case it's a single string)
  if (!is.character(measure)) {
    measure <- as.character(measure)
  }
  
  # Construct the path to the measure's cleaning script
  file_path <- sprintf("./nda/%s/%s.R", api, measure)
  message("\nFetching ", measure, " with nda/", api, "/", measure,".R\n")
  
  # Setup cleanup on exit
  on.exit({
    if (exists("mongo_conn") && !is.null(mongo_conn)) {
      tryCatch({
        mongo_conn$disconnect()
      }, error = function(e) {
        warning(sprintf("Error disconnecting from MongoDB: %s", e$message))
      })
    }
    # Clear the mongo connection from memory
    if (exists("mongo_conn")) {
      rm(mongo_conn)
    }
    gc()  # Force garbage collection
  })
  
  result <- tryCatch({
    base::source(file_path)  # Execute the cleaning script for the measure
    # Apply date format preservation after processing
    # Get the data frame from global environment
    df <- base::get0(measure, envir = .GlobalEnv)
    
    # Only process if df exists and is a data frame
    if (!is.null(df) && is.data.frame(df)) {
      # Reassign the processed data frame
      base::assign(measure, df, envir = .GlobalEnv)
    }
    
    if (api == "qualtrics") {
      # Remove specified qualtrics columns
      cols_to_remove <- c("StartDate", "EndDate", "Status", "Progress", "Duration (in seconds)", 
                          "Finished", "RecordedDate", "ResponseId", "DistributionChannel", 
                          "UserLanguage", "candidateId", "studyId", "measure", "ATTN", "ATTN_1", "SC0")
      df <- df[!names(df) %in% cols_to_remove]
      
      # Reassign the filtered dataframe to the global environment
      base::assign(measure, df, envir = .GlobalEnv)
      
#       source("api/test/ndaCheckQualtricsDuplicates.R")
      ndaCheckQualtricsDuplicates(measure,"qualtrics")
      
      # show missing data that needs filled
      missing_data <- df[is.na(df$src_subject_id) | is.na(df$subjectkey) | is.na(df$interview_age) | is.na(df$interview_date) | is.na(df$sex), ]
      if (nrow(missing_data) > 0) {
        View(missing_data)
      }
     
    }
    
    # Run validation
#     base::source("api/ndaValidator.R")
    validation_results <- ndaValidator(measure, api, limited_dataset)
    
    # Now apply date format preservation AFTER validation
    df <- base::get0(measure, envir = .GlobalEnv)
    # if (!is.null(df) && is.data.frame(df)) {
    #   df <- preserveDateFormat(df, limited_dataset)
    #   base::assign(measure, df, envir = .GlobalEnv)
    # }
    
    # Add limited de-identification summary
    if (limited_dataset == FALSE) {
      message("Dataset has been de-identified using date-shifting and age-capping.")
    }
    
    # audio alert of validation
    ifelse(validation_results$valid, "mario", "wilhelm") |> beepr::beep()
    
#     base::source("api/src/ndaTemplate.R")
    # Create data upload template regardless of if test passes
    ndaTemplate(measure)
    formatElapsedTime(start_time)
    
  }, error = function(e) {
    # Check if identifier is valid (you can modify this logic based on your criteria)
    if (length(identifier) == 0 || all(is.na(identifier))) {
      message("An error occurred: ", e$message)  # General error message
    } else {
      message("Error with ", measure, ": ", e$message)  # Specific error message
    }
    NULL  # Return NULL on error
  })
  
  # Flush environment
#   base::source("api/env/cleanup.R")
  
  return(result)  # Return the result of the processing
}




# Add helper function for MongoDB cleanup
disconnectMongo <- function(mongo) {
  if (!is.null(mongo)) {
    tryCatch({
      mongo$disconnect()
      rm(list = deparse(substitute(mongo)), envir = parent.frame())
    }, error = function(e) {
      warning(sprintf("Error disconnecting from MongoDB: %s", e$message))
    })
  }
}


# Cleanup Function
performCleanup <- function() {
  # Placeholder for cleanup operations, like disconnecting from databases
#   suppressWarnings(source("api/env/cleanup.R"))
}

# Helper function to preserve MM/DD/YYYY format
# preserveDateFormat <- function(df, limited_dataset = limited_dataset) {
#   if ("interview_date" %in% names(df)) {
#     # Convert to Date first to ensure consistent handling
#     dates <- as.Date(df$interview_date, format = "%m/%d/%Y")
#     
#     # Apply format based on limited_dataset flag
#     df$interview_date <- format(dates, 
#                                 ifelse(limited_dataset, "%m/%d/%Y", "%m/01/%Y"))
#     
#     # Add debug message
#     message("Applying date format with limited_dataset = ", limited_dataset)
#     message("Sample dates after formatting: ", 
#             paste(head(df$interview_date), collapse=", "))
#   }
#   return(df)
# }

# Helper function to display time savings.
formatElapsedTime <- function(start_time) {
  time_diff <- Sys.time() - start_time
  units <- attr(time_diff, "units")
  
  formatted_time <- switch(units,
                           "secs" = sprintf("%.1f seconds", time_diff),
                           "mins" = sprintf("%.1f minutes", time_diff),
                           sprintf("%.1f %s", time_diff, units)
  )
  
  message("Formatted for NDA in ", formatted_time, ".")
}

================
File: R/ndaRequiredVariablesExist.R
================
#' Check for Presence of NDA Required Variables in a Data Frame
#'
#' This function checks if a cleaned data frame contains all the variables required by the National Data Archive (NDA).
#' The set of required variables can be adjusted based on the specific requirements of the study and the presence
#' of certain variables like 'visit' or 'week'. The function is useful for ensuring data compliance before submission.
#'
#' @param measure_alias A string representing the alias name of the dataset to be checked.
#' @param measure_type A string indicating the type of measure, used to adjust the list of required variables if necessary.
#' @param nda_required_variables A vector of strings representing the initial set of NDA required variables to check for.
#'        This parameter is overwritten inside the function but can be used to extend the functionality in the future.
#' @return This function does not return a value but uses the `testthat` package to assert the presence of all NDA required variables and provides detailed feedback if any are missing.
#' @export
#' @examples
#' ndaRequiredVariablesExist("your_dataset_alias", "qualtrics", c("src_subject_id", "phenotype"))
#' @importFrom testthat test_that expect_true
#' @importFrom base get setdiff
#' @note While currently the function overwrites the 'nda_required_variables' parameter internally, future versions may allow for dynamic adjustment based on 'measure_type'.
#'       It assumes that the dataset has been cleaned and is named according to a standard naming convention with a '_clean' suffix.
ndaRequiredVariablesExist <- function(measure_alias, measure_type, nda_required_variables) {
  
#   if (!require(testthat)) {install.packages("testthat")}; library(testthat)
  
  # append _clean to the measure in question
  output_df_name <- paste0(measure_alias, "_clean")
  
  # store clean dataframe in df_clean
  df_clean <- base::get(output_df_name)
  
  # Initial list of NDA required variables
  nda_required_variables <- c("src_subject_id", "phenotype", "site", 
                              "subjectkey", "sex", "interview_date", "interview_age")
  
  # Adjust NDA required variables based on presence of 'visit' or 'week'
  adjusted_nda_required <- nda_required_variables
  
  # alter required variables for redcap measures (no interview_date or interview_age)
  if (measure_type=="redcap") {
    adjusted_nda_required <- nda_required_variables <- c("src_subject_id", "phenotype", "site", 
                                                         "subjectkey", "sex")
  }
  
  # If 'visit' and 'week' are not both required, adjust the list accordingly:
  if (!("visit" %in% colnames(df_clean)) && ("week" %in% colnames(df_clean))) {
    adjusted_nda_required <- setdiff(adjusted_nda_required, "visit")  # Remove 'visit' if it's not there but 'week' is
  } else if (("visit" %in% colnames(df_clean)) && !("week" %in% colnames(df_clean))) {
    adjusted_nda_required <- setdiff(adjusted_nda_required, "week")  # Remove 'week' if it's not there but 'visit' is
  } # If neither or both are present, no changes needed to adjusted_nda_required
  
  
  # Now check if the output dataframe contains all adjusted NDA required variables
  missing_vars <- setdiff(adjusted_nda_required, colnames(df_clean))
  
  tryCatch({
    test_that("Check for missing NDA required variables", {
      testthat::expect_true(length(missing_vars) == 0, 
                            info = paste("SCRIPT ERROR: All NDA required variables are not present in '", measure_alias, " please make sure the following variable is present in the clean df: '", missing_vars, "."))
    })
  }, error = function(e) {
    message("All NDA required variables are not present in '", measure_alias, " please make sure the following variable is present in the clean df: '", missing_vars, ".", e$message)
  })
  
}

================
File: R/ndaValidator.R
================
#' Request and Validate Data Against NDA Structure
#'
#' This function takes a measure name and validates the associated dataframe against 
#' the NDA data structure by checking field names, types, and requirements against the NDA API.
#'
#' @param measure_name Character string of the measure name to validate
#' @param api_base_url Character string of the NDA API base URL (optional)
#' @return List containing validation results and any mismatches found
#'
#' @import httr
#' @import jsonlite
#' @import dplyr
#' @noRd

# Function to handle missing required fields
handle_missing_fields <- function(df, elements, missing_required, verbose = FALSE) {
  if(verbose) {
    message("\nAuto-adding missing required fields with missing value codes:")
  }
  
  for (field in missing_required) {
    element <- elements[elements$name == field, ]
    
    if(verbose) {
      cat("\nNotes for", field, ":", element$notes)
    }
    
    # Extract missing value code from notes (e.g. -999)
    missing_code <- NULL
    if (!is.na(element$notes)) {
      # Look for pattern like "999 = Missing" or "999 = Missing/NA"
      if (grepl("\\d+\\s*=\\s*Missing(?:/NA)?", element$notes, perl = TRUE)) {
        value <- gsub(".*?(\\d+)\\s*=\\s*Missing(?:/NA)?.*", "\\1", element$notes, perl = TRUE)
        missing_code <- paste0("-", value)  # Make it negative
      }
    }
    
    if (!is.null(missing_code)) {
      # Convert to proper type and fill entire column
      if (element$type == "Float") {
        df[[field]] <- as.numeric(missing_code)
      } else if (element$type == "Integer") {
        df[[field]] <- as.integer(missing_code)
      } else {
        df[[field]] <- missing_code
      }
      
      if(verbose) {
        cat(sprintf("\n- Added %s with missing code %s as type %s", 
                    field, missing_code, element$type))
      }
    }
  }
  
  if(verbose) cat("\n")
  return(df)
}

# Generic function for field standardization
standardize_field_names <- function(df, measure_name, verbose = FALSE) {
  if(verbose) cat("\nStandardizing common field names...")
  
  # Track all transformations for summary
  transformations <- list()
  
  # Handle index -> trial conversion
  if ("index" %in% names(df)) {
    if(verbose) cat("\n\nProcessing 'index' to 'trial' conversion...")
    
    # Store original state for summary
    orig_values <- head(df$index, 3)
    
    # Convert to numeric if not already
    df$index <- as.numeric(df$index)
    
    # Create trial column
    df$trial <- df$index
    
    # Set non-positive values to NA
    df$trial[df$index <= 0] <- NA
    
    # Count transformations
    total_rows <- length(df$index)
    positive_rows <- sum(df$index > 0, na.rm = TRUE)
    
    # Store transformation summary
    transformations[["index_to_trial"]] <- list(
      from = "index",
      to = "trial",
      total = total_rows,
      valid = positive_rows,
      sample_before = orig_values,
      sample_after = head(df$trial, 3)
    )
    
    if(verbose) {
      cat(sprintf("\n  Total rows: %d", total_rows))
      cat(sprintf("\n  Valid rows: %d", positive_rows))
      cat("\n  Sample values:")
      cat(sprintf("\n    Before: %s", paste(orig_values, collapse=", ")))
      cat(sprintf("\n    After:  %s", paste(head(df$trial, 3), collapse=", ")))
    }
    
    # Remove original column
    df$index <- NULL
  }
  
  # Print summary if any transformations occurred
  if(verbose && length(transformations) > 0) {
    cat("\n\nField standardization summary:")
    for(transform_name in names(transformations)) {
      transform <- transformations[[transform_name]]
      cat(sprintf("\n- %s → %s", transform$from, transform$to))
      cat(sprintf("\n  Processed %d rows (%d valid)",
                  transform$total, transform$valid))
    }
    cat("\n")
  }
  
  return(df)
}

# Extract mapping rules from Notes field
# Modified get_mapping_rules function with better error handling
get_mapping_rules <- function(notes) {
  if (is.null(notes) || is.na(notes) || notes == "") return(NULL)
  
  rules <- list()
  
  tryCatch({
    # Handle array notation like "1=(0.9, 0.5, 0.1)"
    if (grepl("=\\(.*\\)", notes)) {
      pattern_matches <- gregexpr("(\\d+)=\\(([^)]+)\\)", notes)
      if (pattern_matches[[1]][1] != -1) {
        matches <- regmatches(notes, pattern_matches)[[1]]
        for (match in matches) {
          code_match <- regexec("(\\d+)=\\(([^)]+)\\)", match)
          parts <- regmatches(match, code_match)[[1]]
          if (length(parts) >= 3) {  # Check if we have enough parts
            code <- parts[2]
            values <- sprintf("[%s]", parts[3])
            rules[[values]] <- code
          }
        }
      }
    }
    
    # Handle simple mappings like "1=Red" and "NaN=-1"
    if (grepl("[^=]+=[^;]+", notes)) {
      patterns <- strsplit(notes, ";\\s*")[[1]]
      for (pattern in patterns) {
        if (grepl("=", pattern)) {
          parts <- strsplit(pattern, "=")[[1]]
          if (length(parts) >= 2) {  # Check if we have both parts
            value <- trimws(parts[1])
            code <- trimws(parts[2])
            rules[[code]] <- value
          }
        }
      }
    }
  }, error = function(e) {
    warning(sprintf("Error parsing mapping rules: %s\nNotes: %s", e$message, notes))
    return(list())  # Return empty list on error instead of NULL
  })
  
  return(rules)
}

# semi-generalizable e.g. handle mooney rt null
apply_null_transformations <- function(df, elements) {
  for (i in 1:nrow(elements)) {
    field_name <- elements$name[i]
    type <- elements$type[i]
    notes <- elements$notes[i]
    
    if (field_name %in% names(df) && !is.null(notes)) {
      # Extract transformation rules from Notes
      rules <- get_mapping_rules(notes)
      
      if (!is.null(rules) && length(rules) > 0) {
        cat(sprintf("\nRules for field '%s':\n", field_name))
        print(rules)
        
        null_placeholder <- as.numeric(rules[[1]])
        
        cat(sprintf("Using placeholder value: %s\n", null_placeholder))
        
        # Add debugging before conversion
        cat(sprintf("\nUnique values before conversion in %s:\n", field_name))
        print(unique(df[[field_name]]))
        
        message("applying type conversions")
        
        df[[field_name]] <- as.character(df[[field_name]])
        
        null_mask <- df[[field_name]] %in% c("null", "NaN", "") | is.na(df[[field_name]])
        df[[field_name]][null_mask] <- null_placeholder
        
        # Add debugging for type conversion
        if (type == "Integer" || type == "Float") {
          cat(sprintf("\nConverting %s to %s\n", field_name, type))
          # Check for problematic values before conversion
          non_numeric <- df[[field_name]][!grepl("^-?\\d*\\.?\\d+$", df[[field_name]])]
          if (length(non_numeric) > 0) {
            cat(sprintf("Warning: Non-numeric values found in %s:\n", field_name))
            print(unique(non_numeric))
          }
          
          if (type == "Integer") {
            df[[field_name]] <- as.integer(df[[field_name]])
          } else if (type == "Float") {
            df[[field_name]] <- as.numeric(df[[field_name]])
          }
          
          # Check for NAs after conversion
          new_nas <- is.na(df[[field_name]])
          if (any(new_nas)) {
            cat(sprintf("\nWarning: %d NAs introduced in %s\n", sum(new_nas), field_name))
            cat("Sample of values that became NA:\n")
            print(head(df[[field_name]][new_nas]))
          }
        }
        
        cat(sprintf("Values after transformation: %s\n", 
                    paste(unique(df[[field_name]]), collapse = ", ")))
      }
    }
  }
  return(df)
}



# Convert fields to their proper type based on NDA definition
# Modify the apply_type_conversions function to be more robust
# Convert fields to proper type based on NDA definition
apply_type_conversions <- function(df, elements, verbose = FALSE) {
  if(verbose) cat("\nApplying type conversions...")
  conversion_summary <- list()
  
  for (i in 1:nrow(elements)) {
    field_name <- elements$name[i]
    type <- elements$type[i]
    
    if (field_name %in% names(df) && !is.null(type)) {
      tryCatch({
        if (type %in% c("Integer", "Float")) {
          if(verbose) cat(sprintf("\n\nField: %s", field_name))
          if(verbose) cat(sprintf("\n  Target type: %s", type))
          
          # Store original values for comparison
          orig_values <- head(df[[field_name]], 3)
          
          # First convert to character
          df[[field_name]] <- as.character(df[[field_name]])
          
          # Remove currency symbols, commas, etc
          df[[field_name]] <- gsub("[^0-9.-]", "", df[[field_name]])
          
          if (type == "Integer") {
            # Convert to numeric first to handle decimals
            df[[field_name]] <- as.numeric(df[[field_name]])
            
            # Check for decimal values
            float_mask <- !is.na(df[[field_name]]) & 
              abs(df[[field_name]] - floor(df[[field_name]])) > 0
            
            if (any(float_mask)) {
              float_count <- sum(float_mask)
              if(verbose) {
                cat(sprintf("\n  Found %d decimal values to round", float_count))
                cat("\n  Sample conversions:")
                float_examples <- head(df[[field_name]][float_mask])
                rounded_examples <- round(float_examples)
                for(j in seq_along(float_examples)) {
                  cat(sprintf("\n    %.2f → %d", 
                              float_examples[j], 
                              rounded_examples[j]))
                }
              }
              df[[field_name]] <- round(df[[field_name]])
            }
            
            df[[field_name]] <- as.integer(df[[field_name]])
            
          } else if (type == "Float") {
            df[[field_name]] <- as.numeric(df[[field_name]])
          }
          
          # Check for NAs after conversion
          na_count <- sum(is.na(df[[field_name]]))
          if (na_count > 0 && verbose) {
            cat(sprintf("\n  Warning: %d NA values introduced", na_count))
            cat("\n  Sample values that became NA:")
            na_mask <- is.na(df[[field_name]])
            cat(sprintf("\n    Original: %s", 
                        paste(head(orig_values[na_mask]), collapse=", ")))
          }
          
          # Store summary for this field
          conversion_summary[[field_name]] <- list(
            type = type,
            nas_introduced = na_count,
            sample_before = head(orig_values),
            sample_after = head(df[[field_name]])
          )
        }
      }, error = function(e) {
        if(verbose) {
          cat(sprintf("\n\nError converting %s to %s:", field_name, type))
          cat(sprintf("\n  %s", e$message))
        }
      })
    }
  }
  
  if(verbose && length(conversion_summary) > 0) {
    cat("\n\nType conversion summary:")
    for(field in names(conversion_summary)) {
      cat(sprintf("\n- %s → %s", field, conversion_summary[[field]]$type))
      if(conversion_summary[[field]]$nas_introduced > 0) {
        cat(sprintf(" (%d NAs)", conversion_summary[[field]]$nas_introduced))
      }
    }
    cat("\n")
  }
  
  return(df)
}

# Demonstrate with standardize_dates as well
standardize_dates <- function(df, date_cols = c("interview_date"), verbose = TRUE, limited_dataset = limited_dataset) {
  date_summary <- list()
  
  for (col in date_cols) {
    if (col %in% names(df)) {
      tryCatch({
        if(verbose) cat(sprintf("\n\nField: %s", col))
        
        # Store original values
        orig_dates <- head(df[[col]], 3)
        
        # Remove timezone information
        dates <- gsub("\\s+\\d{2}:\\d{2}:\\d{2}.*$", "", df[[col]])
        
        # Try different date formats
        date_formats <- c(
          "%Y-%m-%d",    # 2023-12-31
          "%m/%d/%Y",    # 12/31/2023
          "%Y/%m/%d",    # 2023/12/31
          "%d-%m-%Y",    # 31-12-2023
          "%m-%d-%Y"     # 12-31-2023
        )
        
        success <- FALSE
        for (format in date_formats) {
          parsed_dates <- tryCatch({
            as.Date(dates, format = format)
          }, error = function(e) NULL)
          
          if (!is.null(parsed_dates) && !all(is.na(parsed_dates))) {
            if(verbose) cat(sprintf("\n  Detected format: %s", format))
            # df[[col]] <- format(parsed_dates, "%Y-%m-%d")
            #df[[col]] <- format(parsed_dates, "%m/%d/%Y")
            # Perform interview_date date shifting to created de-identified dataset
            df[[col]] <- format(parsed_dates, ifelse(limited_dataset, "%m/%d/%Y", "%m/01/%Y"))
            
            success <- TRUE
            
            date_summary[[col]] <- list(
              original_format = format,
              sample_before = orig_dates,
              sample_after = head(df[[col]], 3)
            )
            break
          }
        }
        
        if(!success && verbose) {
          cat(sprintf("\n  Warning: Could not determine date format"))
          cat(sprintf("\n  Sample values: %s", 
                      paste(head(dates), collapse=", ")))
        }
        
      }, error = function(e) {
        if(verbose) {
          cat(sprintf("\n\nError processing dates in %s:", col))
          cat(sprintf("\n  %s", e$message))
        }
      })
    }
  }
  
  if(verbose && length(date_summary) > 0) {
    if(limited_dataset == FALSE) message("\n\nDe-identifying interview_date using date-shifting...")
    cat("Date standardization summary:")
    for(field in names(date_summary)) {
      cat(sprintf("\n- %s", field))
      cat(sprintf("\n  Before: %s", 
                  paste(date_summary[[field]]$sample_before, collapse=", ")))
      cat(sprintf("\n  After:  %s", 
                  paste(date_summary[[field]]$sample_after, collapse=", ")))
    }
    cat("\n")
  }
  
  return(df)
}

standardize_age <- function(df, verbose = TRUE, limited_dataset = limited_dataset) {
  if ("interview_age" %in% names(df) && limited_dataset == FALSE) {
    if(verbose && limited_dataset == FALSE) message("\nDe-identifying interview_age using age-capping...")
    
    # Convert to numeric first
    df$interview_age <- as.numeric(df$interview_age)
    orig_age_stats <- summary(df$interview_age)
    
    # Count values that will be changed
    values_to_change <- sum(df$interview_age > 1068, na.rm = TRUE)
    
    # Apply the age standardization (cap at 1068 months = 89 years * 12)
    df$interview_age <- pmin(df$interview_age, 1068)
    
    if(verbose) {
      cat("Age standardization summary:")
      cat("\nBefore:", capture.output(orig_age_stats))
      cat("\nAfter:", capture.output(summary(df$interview_age)))
      if(values_to_change > 0) {
        cat(sprintf("\nNumber of values capped at 1068 months: %d", values_to_change))
      } else {
        cat("\nNo values needed capping (all were <= 1068 months)")
      }
    }
  }
  
  return(df)
}

# Calculate Levenshtein distance similarity between two strings
calculate_similarity <- function(str1, str2) {
  # Convert to lowercase
  str1 <- tolower(str1)
  str2 <- tolower(str2)
  
  # Create matrix
  m <- nchar(str1)
  n <- nchar(str2)
  d <- matrix(0, nrow = m + 1, ncol = n + 1)
  d[1,] <- 0:n
  d[,1] <- 0:m
  
  # Fill matrix
  for (i in 2:(m + 1)) {
    for (j in 2:(n + 1)) {
      cost <- if (substr(str1, i-1, i-1) == substr(str2, j-1, j-1)) 0 else 1
      d[i,j] <- min(
        d[i-1,j] + 1,      # deletion 
        d[i,j-1] + 1,      # insertion
        d[i-1,j-1] + cost  # substitution
      )
    }
  }
  
  # Return similarity score (1 - normalized distance)
  return(1 - d[m+1,n+1] / max(m,n))
}

# Helper function to standardize handedness values
standardize_handedness <- function(value) {
  # Create mapping for handedness terms
  handedness_map <- c(
    "left" = "L",
    "l" = "L",
    "right" = "R",
    "r" = "R",
    "both" = "B",
    "ambidextrous" = "B"
  )
  
  # Convert to lowercase for consistent matching
  value <- tolower(value)
  
  # Map values using the handedness_map
  mapped_values <- handedness_map[value]
  mapped_values[is.na(mapped_values)] <- value[is.na(mapped_values)]
  
  # Count and report transformations
  # n_transformed <- sum(value != mapped_values, na.rm = TRUE)
  # if (n_transformed > 0) {
  #   cat(sprintf("\nTransformed %d handedness values to NDA standard format\n", n_transformed))
  # }
  
  return(mapped_values)
}

# Helper function to standardize boolean to numeric values
standardize_binary <- function(value) {
  # Create mapping for boolean to numeric terms (including case variations)
  binary_map <- c(
    "true" = "1",
    "false" = "0",
    "t" = "1",
    "f" = "0",
    "TRUE" = "1",
    "FALSE" = "0",
    "True" = "1",
    "False" = "0"
  )
  
  # Convert value to character without changing case
  value <- as.character(value)
  
  # Map values using the binary_map (exact match)
  mapped_values <- binary_map[value]
  
  # For any unmatched values, try lowercase matching
  still_na <- is.na(mapped_values)
  if(any(still_na)) {
    mapped_values[still_na] <- binary_map[tolower(value[still_na])]
  }
  
  # Keep original values for any remaining unmatched
  mapped_values[is.na(mapped_values)] <- value[is.na(mapped_values)]
  
  # Count and report transformations
  n_transformed <- sum(value != mapped_values, na.rm = TRUE)
  if (n_transformed > 0) {
    cat(sprintf("\nTransformed %d boolean values to 0/1 format\n", n_transformed))
  }
  
  return(mapped_values)
}

# Parse array-like strings to vectors
parse_array_string <- function(value) {
  if (is.null(value) || is.na(value)) return(NULL)
  
  # Handle string arrays
  if (is.character(value)) {
    # Remove unicode prefix, brackets, and quotes
    clean_str <- gsub("\\[|\\]|u'|'", "", value)
    values <- strsplit(clean_str, ",\\s*")[[1]]
    return(tolower(trimws(values)))
  }
  
  # Handle numeric arrays
  if (is.numeric(value) && length(value) > 1) {
    return(sprintf("%.1f", value))
  }
  
  return(tolower(trimws(value)))
}

# Helper function to fetch structure elements from API
fetch_structure_elements <- function(structure_name, api_base_url) {
  
#   if (!require(httr)) {install.packages("httr")}; library(httr)
#   if (!require(jsonlite)) {install.packages("jsonlite")}; library(jsonlite)
  
  url <- sprintf("%s/datastructure/%s", api_base_url, structure_name)
  response <- httr::GET(url)
  
  if (status_code(response) != 200) {
    stop("Failed to fetch structure elements: ", content(response, "text"))
  }
  
  content <- jsonlite::fromJSON(rawToChar(response$content))
  
  if (!"dataElements" %in% names(content)) {
    stop("Unexpected API response format - no dataElements found")
  }
  
  elements <- content$dataElements
  return(elements)
}

# Calculate similarity with more accurate prefix handling
# Calculate similarity with more accurate prefix handling
find_and_rename_fields <- function(df, elements, structure_name, verbose = TRUE) {
  renamed <- list(
    df = df,
    renames = character(),
    columns_to_drop = character(),
    similarity_scores = list()
  )
  
  # Get dataframe column names
  df_cols <- names(df)
  valid_fields <- elements$name
  
  # Get structure short name by taking last 2 digits of structure_name
  structure_prefix <- substr(structure_name, nchar(structure_name) - 1, nchar(structure_name))
  
  # Find unknown fields
  unknown_fields <- setdiff(df_cols, valid_fields)
  
  if (length(unknown_fields) > 0) {
    if(verbose) cat("\nAnalyzing field name similarities...\n")
    
    for (field in unknown_fields) {
      # Check if this is a hierarchical field (contains multiple underscores with numbers)
      parts <- strsplit(field, "_")[[1]]
      num_parts <- sum(grepl("^\\d+$", parts))
      
      # If field has more than 2 numeric parts, it's hierarchical - skip renaming
      if (num_parts > 2) {
        if(verbose) {
          cat(sprintf("\nField: %s\n", field))
          cat("Skipping rename - hierarchical field structure detected\n")
        }
        next
      }
      
      # For non-hierarchical fields, proceed with similarity matching
      base_field <- sub(paste0("^", structure_prefix, "_"), "", field)
      
      # Calculate similarity scores
      similarities <- sapply(valid_fields, function(name) {
        # Remove prefix from target field if it exists
        target_base <- sub(paste0("^", structure_prefix, "_"), "", name)
        
        # Calculate direct similarity
        calculate_similarity(field, name)
      })
      
      # Store all similarity scores
      renamed$similarity_scores[[field]] <- sort(similarities, decreasing = TRUE)
      
      if(verbose) {
        cat(sprintf("\nField: %s\n", field))
        cat("Top matches:\n")
        top_matches <- head(sort(similarities, decreasing = TRUE), 3)
        for(i in seq_along(top_matches)) {
          cat(sprintf("%d. %s (%.2f%% match)\n",
                     i,
                     names(top_matches)[i],
                     top_matches[i] * 100))
        }
      }
      
      # Remove any NA values
      similarities <- similarities[!is.na(similarities)]
      
      if (length(similarities) > 0) {
        best_match <- names(similarities)[which.max(similarities)]
        best_score <- max(similarities)
        
        if (best_score > 0.9) {  # Increased threshold for more conservative matching
          if(verbose) {
            message(sprintf("\nRENAMING: '%s' to '%s' (similarity: %.2f%%)\n",
                          field, best_match, best_score * 100))
          }
          
          # Add the new column with renamed data
          renamed$df[[best_match]] <- renamed$df[[field]]
          
          # Mark original column for dropping
          renamed$columns_to_drop <- c(renamed$columns_to_drop, field)
          
          # Store the rename operation
          renamed$renames <- c(renamed$renames,
                             sprintf("%s -> %s (%.2f%%)",
                                   field, best_match, best_score * 100))
        } else if(verbose) {
          cat(sprintf("No automatic rename - best match below 90%% threshold\n"))
        }
      }
    }
    
    # Drop original columns after all renames
    if(length(renamed$columns_to_drop) > 0) {
      if(verbose) {
        cat("\nDropping original columns:")
        cat(sprintf("\n  %s", paste(renamed$columns_to_drop, collapse=", ")))
      }
      renamed$df <- renamed$df[, !names(renamed$df) %in% renamed$columns_to_drop]
    }
    
    if(verbose && length(renamed$renames) > 0) {
      cat("\n\nRename operations completed:")
      cat(paste("\n-", renamed$renames), sep = "")
      cat("\n")
    }
  }
  
  return(renamed)
}

# Helper function to get violating values with type conversion
# Updated get_violations function with more robust categorical matching
# Updated get_violations function
# Updated get_violations function with special handling for ranges with both :: and ;
get_violations <- function(value, range_str) {
  if (is.null(range_str) || is.na(range_str) || range_str == "") return(character(0))
  
  # Special case for mixed ranges like "1::26;77"
  if (grepl("::", range_str) && grepl(";", range_str)) {
    # Split by semicolon first
    parts <- strsplit(range_str, ";")[[1]]
    
    # Get the numeric range from the first part
    range_part <- parts[grepl("::", parts)][1]
    range <- as.numeric(strsplit(range_part, "::")[[1]])
    
    # Get individual values from other parts
    individual_values <- as.numeric(parts[!grepl("::", parts)])
    
    # Combine valid values: numbers in range plus individual values
    valid_values <- c(seq(from = range[1], to = range[2]), individual_values)
    
    # Check for violations
    invalid_mask <- !value %in% valid_values
    invalid_mask[is.na(invalid_mask)] <- FALSE
    
    return(sort(unique(value[invalid_mask])))
  }
  
  # Rest of the function for simple ranges
  if (grepl("::", range_str)) {
    # Numeric range check
    range <- as.numeric(strsplit(range_str, "::")[[1]])
    
    # Convert value to numeric if it's character
    if (is.character(value)) {
      value <- as.numeric(value)
    }
    
    invalid_mask <- value < range[1] | value > range[2]
    invalid_mask[is.na(invalid_mask)] <- FALSE
    return(sort(unique(value[invalid_mask])))
  } else if (grepl(";", range_str)) {
    # Categorical values
    valid_values <- trimws(strsplit(range_str, ";")[[1]])
    
    # Convert to character for comparison
    value_char <- as.character(value)
    
    invalid_mask <- !value_char %in% valid_values
    invalid_mask[is.na(invalid_mask)] <- FALSE
    return(sort(unique(value[invalid_mask])))
  }
  
  return(character(0))
}

# Main validation logic function
# Modified validate_structure function with better error handling
validate_structure <- function(df, elements, measure_name, verbose = FALSE) {
  if(verbose) cat("\nValidating data structure...")
  
  results <- list(
    valid = TRUE,
    missing_required = character(0),
    value_range_violations = list(),
    unknown_fields = character(0),
    warnings = character(0)
  )
  
  tryCatch({
    # Get field lists
    required_fields <- elements$name[elements$required == "Required"]
    valid_fields <- elements$name
    df_cols <- names(df)
    
    # # Check for and DROP unknown fields    
    # results$unknown_fields <- setdiff(df_cols, valid_fields)
    # if(length(results$unknown_fields) > 0) {
    #   if(verbose) {
    #     cat("\n\nUnknown fields detected:")
    #     cat(sprintf("\n  %s", paste(results$unknown_fields, collapse=", ")))
    #   }
    #   df <- df[, !names(df) %in% results$unknown_fields]
    #   results$unknown_fields <- character(0)
    #   if(verbose) {
    #     cat("\nDropped unknown fields from dataframe")
    #   }
    #   assign(measure_name, df, envir = .GlobalEnv)
    # }
    
    # Check for unknown fields
    results$unknown_fields <- setdiff(df_cols, valid_fields)
    if(length(results$unknown_fields) > 0) {
      if(verbose) {
        cat("\n\nUnknown fields detected:")
        cat(sprintf("\n  %s", paste(results$unknown_fields, collapse=", ")))
      }
      # No longer dropping fields here
      results$valid <- FALSE  # Keep failing validation if unknown fields exist
    }
    
    # Update field lists after renaming
    df_cols <- names(df)  # Get updated column names
    required_fields <- setdiff(required_fields, df_cols)  # Only keep the ones that are still missing
    
    # Check required fields 
    if(length(required_fields) > 0) {
      missing_required <- required_fields  # These are already the missing ones now
      if(length(missing_required) > 0) {
        results$valid <- FALSE
        results$missing_required <- missing_required
        if(verbose) {
          cat("\n\nMissing required fields:")
          cat(sprintf("\n  %s", paste(missing_required, collapse=", ")))
        }
      } else if(verbose) {
        cat("\n\nAll required fields present")
      }
    }
    
    # Rest of validation code...
    
    # Check value ranges
    if(verbose) cat("\n\nChecking value ranges...")
    
    for(col in intersect(df_cols, valid_fields)) {
      element <- elements[elements$name == col, ]
      
      if(nrow(element) > 0 && 
         !is.null(element$valueRange) && 
         !is.na(element$valueRange) && 
         element$valueRange != "") {
        
        if(verbose) {
          cat(sprintf("\n\nField: %s", col))
          cat(sprintf("\n  Expected range: %s", element$valueRange))
        }
        
        # Handle binary fields
        if(element$valueRange == "0;1") {
          values <- as.character(df[[col]])
          if(any(tolower(values) %in% c("true", "false"))) {
            if(verbose) cat("\n  Converting boolean values to 0/1")
            df[[col]] <- standardize_binary(values)
            assign(measure_name, df, envir = .GlobalEnv)
          }
        }
        
        # Check for violations
        violating_values <- tryCatch({
          get_violations(df[[col]], element$valueRange)
        }, error = function(e) {
          results$warnings <- c(
            results$warnings,
            sprintf("Error checking %s: %s", col, e$message)
          )
          character(0)
        })
        
        if(length(violating_values) > 0) {
          results$valid <- FALSE
          results$value_range_violations[[col]] <- list(
            expected = element$valueRange,
            actual = violating_values
          )
          
          if(verbose) {
            cat("\n  Value range violations found:")
            cat(sprintf("\n    Invalid values: %s", 
                        paste(head(violating_values, 5), collapse=", ")))
            if(length(violating_values) > 5) {
              cat(sprintf(" (and %d more...)", 
                          length(violating_values) - 5))
            }
          }
        } else if(verbose) {
          cat("\n  All values within expected range")
        }
      }
    }
    
    # Final summary
    if(verbose) {
      message("\n\nValidation Summary:")
      message(sprintf("- Status: %s", 
                      if(results$valid) "PASSED" else "FAILED"))
      
      if(length(results$unknown_fields) > 0) {
        message(sprintf("- Unknown fields: %d", 
                        length(results$unknown_fields)))
      }
      
      if(length(results$missing_required) > 0) {
        message(sprintf("- Missing required fields: %d", 
                        length(results$missing_required)))
      }
      
      if(length(results$value_range_violations) > 0) {
        message(sprintf("- Fields with range violations: %d", 
                        length(results$value_range_violations)))
      }
      
      if(length(results$warnings) > 0) {
        message("\n\nWarnings:")
        for(warning in results$warnings) {
          message(sprintf("\n- %s", warning))
        }
      }
      cat("\n")
    }
    
  }, error = function(e) {
    results$valid <- FALSE
    results$warnings <- c(
      results$warnings,
      sprintf("Critical validation error: %s", e$message)
    )
    
    if(verbose) {
      cat("\n\nCritical Validation Error:")
      cat(sprintf("\n  %s", e$message))
    }
  })
  
  return(results)
}


# Modify the main validation function to include date standardization
# Add enhanced debug logging
debug_print <- function(msg, df = NULL, sample_size = 5, debug = FALSE) {
  if(debug) {
    cat("\nDEBUG:", msg, "\n")
    if (!is.null(df)) {
      cat("Dataframe info:\n")
      cat("- Dimensions:", paste(dim(df), collapse=" x "), "\n")
      cat("- Column names:", paste(names(df), collapse=", "), "\n")
      cat("- First", sample_size, "rows of data:\n")
      print(head(df, sample_size))
    }
  }
}


# Modified ndaValidator with enhanced error handling
# Helper function to standardize column names
standardize_column_names <- function(df, structure_name, verbose = FALSE) {
  if(verbose) cat("\nStandardizing column names...")
  
  # Get structure short name by taking last 2 digits of structure_name
  prefix <- substr(structure_name, nchar(structure_name) - 1, nchar(structure_name))
  
  # Create name mapping function
  standardize_name <- function(name) {
    # Convert to lowercase
    name <- tolower(name)
    # Replace hyphens with underscores
    name <- gsub("-", "_", name)
    # Handle prefix if present
    if (grepl(paste0("^", prefix, "[_-]?\\d+$"), name)) {
      # Ensure consistent underscore between prefix and number
      name <- gsub(paste0("^(", prefix, ")[_-]?(\\d+)$"), "\\1_\\2", name)
    }
    return(name)
  }
  
  # Standardize column names
  old_names <- names(df)
  new_names <- sapply(old_names, standardize_name)
  
  # Report changes
  changed <- old_names != new_names
  if (any(changed) && verbose) {
    cat("\n\nColumn name changes:")
    for (i in which(changed)) {
      cat(sprintf("\n  %s → %s", old_names[i], new_names[i]))
    }
    
    # Add summary
    cat(sprintf("\n\nSummary: %d names standardized\n", sum(changed)))
  }
  
  # Apply new names
  names(df) <- new_names
  return(df)
}

parse_field_name <- function(name) {
  # Split name into components
  parts <- strsplit(name, "_")[[1]]
  
  # Extract prefix and numeric components
  prefix <- parts[1]  # e.g., "lec"
  
  # Get all numeric components
  numbers <- as.numeric(grep("^\\d+$", parts, value = TRUE))
  
  list(
    prefix = prefix,
    numbers = numbers,
    original = name
  )
}

# Helper function to compare numeric patterns
compare_numeric_patterns <- function(name1, name2) {
  # Parse both names
  parsed1 <- parse_field_name(name1)
  parsed2 <- parse_field_name(name2)
  
  # Must have same prefix
  if (parsed1$prefix != parsed2$prefix) {
    return(0)
  }
  
  # Compare number of numeric components
  n1 <- length(parsed1$numbers)
  n2 <- length(parsed2$numbers)
  
  # If one is hierarchical (has underscore numbers) and other isn't, they're different
  if ((grepl("_\\d+_\\d+", name1) && !grepl("_\\d+_\\d+", name2)) ||
      (!grepl("_\\d+_\\d+", name1) && grepl("_\\d+_\\d+", name2))) {
    return(0.3)  # Very low similarity for different patterns
  }
  
  # Compare the actual numbers
  max_nums <- max(n1, n2)
  matching_nums <- sum(parsed1$numbers[1:min(n1, n2)] == parsed2$numbers[1:min(n1, n2)])
  
  # Calculate similarity based on matching numbers
  similarity <- matching_nums / max_nums
  
  return(similarity)
}

# Modify transform_value_ranges to be more robust
transform_value_ranges <- function(df, elements, verbose = FALSE) {
  if(verbose) cat("\nChecking and transforming value ranges...")
  range_summary <- list()
  
  # Check which columns are required
  required_fields <- elements$name[elements$required == "Required"]
  
  # MODIFIED: More robust check for missing/NA values in required fields
  missing_required <- FALSE
  missing_fields <- character(0)
  
  for(field in required_fields) {
    if(field %in% names(df)) {
      if(any(is.na(df[[field]]) | df[[field]] == "")) {
        missing_required <- TRUE
        missing_fields <- c(missing_fields, field)
      }
    } else {
      missing_required <- TRUE
      missing_fields <- c(missing_fields, field)
    }
  }
  
  if(missing_required) {
    stop(sprintf('\nNDA required values contain NA or no data in fields: %s\nPlease update ndar_subject01 values to make sure no data is missing', 
                 paste(missing_fields, collapse=", ")))
  }
  
  # Rest of the function remains the same
  # Only check non-required columns for emptiness e.g."rt" on eefrt  
  empty_cols <- sapply(df[, !names(df) %in% required_fields], function(col) all(is.na(col) | col == ""))
  if (any(empty_cols)) {
    empty_col_names <- names(empty_cols)[empty_cols]
    if(verbose) {
      cat("\n\nEmpty columns detected:")
      cat(sprintf("\n  Dropping: %s", paste(empty_col_names, collapse=", ")))
    }
    df <- df[, !names(df) %in% empty_col_names, drop=FALSE]
  }
  
  # Process binary fields first
  binary_fields <- elements$name[!is.na(elements$valueRange) & elements$valueRange == "0;1"]
  if (length(binary_fields) > 0) {
    if(verbose) cat("\n\nProcessing binary fields (0;1)...")
    
    for (field in binary_fields) {
      if (field %in% names(df)) {
        values <- as.character(df[[field]])
        potential_booleans <- c("true", "false", "t", "f", "TRUE", "FALSE", "True", "False")
        if (any(values %in% potential_booleans, na.rm = TRUE)) {
          if(verbose) {
            cat(sprintf("\n\nField: %s", field))
            cat("\n  Converting boolean values to 0/1")
          }
          
          # Store original values
          orig_values <- unique(values)
          
          # Transform values 
          df[[field]] <- standardize_binary(values)
          
          # Store summary
          range_summary[[field]] <- list(
            type = "binary",
            values_transformed = sum(values != df[[field]], na.rm = TRUE),
            orig_values = orig_values,
            new_values = unique(df[[field]])
          )
          
          if(verbose) {
            cat("\n  Value mapping:")
            cat(sprintf("\n    Before: %s", paste(head(orig_values), collapse=", ")))
            cat(sprintf("\n    After:  %s", paste(head(unique(df[[field]])), collapse=", ")))
          }
        }
      }
    }
  }
  
  # Process fields that have value range rules
  for (i in 1:nrow(elements)) {
    field_name <- elements$name[i]
    value_range <- elements$valueRange[i]
    if (field_name %in% names(df) && !is.na(value_range) && value_range != "" && 
        value_range != "0;1" && grepl(";", value_range)) {
      
      if(verbose) {
        cat(sprintf("\n\nField: %s", field_name))
        cat(sprintf("\n  Expected values: %s", value_range))
      }
      
      # Store original values
      orig_values <- unique(df[[field_name]])
      
      # Get expected values and standardize
      expected_values <- trimws(unlist(strsplit(value_range, ";")))
      current_values <- df[[field_name]]
      transformed_count <- 0
      
      # Special handling for handedness
      if(field_name == "handedness") {
        df[[field_name]] <- standardize_handedness(current_values)
        transformed_count <- sum(current_values != df[[field_name]], na.rm = TRUE)
      } else {
        # Map case-insensitive matches for other fields
        for (exp_val in expected_values) {
          matches <- tolower(current_values) == tolower(exp_val) 
          if (any(matches)) {
            transformed_count <- transformed_count + sum(matches)
            df[[field_name]][matches] <- exp_val
          }
        }
      }
      
      # Store summary
      range_summary[[field_name]] <- list(
        type = "categorical",
        values_transformed = transformed_count,
        orig_values = orig_values,
        new_values = unique(df[[field_name]])
      )
      
      if(verbose && transformed_count > 0) {
        cat(sprintf("\n  Transformed %d values", transformed_count))
        cat("\n  Value comparison:")
        cat(sprintf("\n    Before: %s", paste(head(orig_values), collapse=", ")))
        cat(sprintf("\n    After:  %s", paste(head(unique(df[[field_name]])), collapse=", ")))
      }
    }   
  }
  
  # Print summary if needed
  if(verbose && length(range_summary) > 0) {
    cat("\n\nValue range transformation summary:")
    for(field in names(range_summary)) {
      cat(sprintf("\n- %s", field))
      if(range_summary[[field]]$values_transformed > 0) {
        cat(sprintf(" (%d values standardized)", range_summary[[field]]$values_transformed))
      }
    }
    cat("\n")
  }
  
  return(df)
}

# Modified ndaValidator to include column name standardization
ndaValidator <- function(measure_name,
                         source,
                         limited_dataset = FALSE,
#                          api_base_url = "https://nda.nih.gov/api/datadictionary/v2",
                         verbose = TRUE,
                         debug = FALSE) {
  tryCatch({
    # Get the dataframe from the global environment
    df <- base::get(measure_name, envir = .GlobalEnv)
    debug_print("Initial dataframe loaded", df, debug = debug)
    
    # Get structure name
    structure_name <- measure_name
    
    # Add explicit date standardization step to make data de-identified
    df <- standardize_dates(df, verbose = verbose, limited_dataset = limited_dataset)
    
    # Add explicit age standardization step to make data de-identified
    df <- standardize_age(df, verbose = verbose, limited_dataset = limited_dataset)
    
    # Standardize column names based on structure
    df <- standardize_column_names(df, structure_name, verbose = verbose)
    
    # Add field name standardization
    df <- standardize_field_names(df, measure_name, verbose = verbose)
    
    # Save standardized dataframe back to global environment
    assign(measure_name, df, envir = .GlobalEnv)
    
    # Continue with structure fetching and validation...
    message("\n\nFetching ", structure_name, " Data Structure from NDA API...")
    elements <- fetch_structure_elements(structure_name, api_base_url)
    
    if (is.null(elements) || nrow(elements) == 0) {
      stop("No elements found in the structure definition")
    }
    
    # ADD THE BLOCK HERE - BEFORE ANY OTHER TRANSFORMATIONS
    # Check for required fields that are missing and add them
    required_fields <- elements$name[elements$required == "Required"]
    missing_required <- required_fields[!required_fields %in% names(df)]
    if(length(missing_required) > 0) {
      df <- handle_missing_fields(df, elements, missing_required, verbose = TRUE)
      assign(measure_name, df, envir = .GlobalEnv)
    }
    
    # Then continue with the rest of the transformations
    renamed_results <- find_and_rename_fields(df, elements, structure_name, verbose)
    df <- renamed_results$df
    
    # Process the dataframe with additional error handling
    df <- tryCatch({
      # Apply type conversions
      df <- apply_type_conversions(df, elements, verbose = verbose)
      
      # Apply null transformations
      df <- apply_null_transformations(df, elements, verbose = verbose)
      
      # Transform value ranges
      df <- transform_value_ranges(df, elements, verbose = verbose)
      
      df
    }, error = function(e) {
      stop(sprintf("Error processing dataframe: %s", e$message))
    })
    
    # Save processed dataframe back to global environment
    assign(measure_name, df, envir = .GlobalEnv)
    
    # Check and add any missing required fields BEFORE validation
    required_fields <- elements$name[elements$required == "Required"]
    missing_required <- required_fields[!required_fields %in% names(df)]
    if(length(missing_required) > 0) {
      df <- handle_missing_fields(df, elements, missing_required, verbose = TRUE)
      assign(measure_name, df, envir = .GlobalEnv)
    }
    
    # Now validate the complete dataset
    validation_results <- validate_structure(df, elements, measure_name, verbose = verbose)
    return(validation_results)
    
  }, error = function(e) {
    message("Error: ", e$message)
    return(NULL)
  })
}

# Modified apply_null_transformations with better error handling
apply_null_transformations <- function(df, elements, verbose = FALSE) {
  if(verbose) cat("\nApplying null value transformations...")
  transform_summary <- list()
  
  for (i in 1:nrow(elements)) {
    field_name <- elements$name[i]
    type <- elements$type[i]
    notes <- elements$notes[i]
    
    if (field_name %in% names(df) && !is.null(notes)) {
      tryCatch({
        # Extract transformation rules from Notes
        rules <- get_mapping_rules(notes)
        
        if (!is.null(rules) && length(rules) > 0) {
          if(verbose) {
            cat(sprintf("\n\nField: %s", field_name))
            cat("\n  Rules found:")
            for(rule_name in names(rules)) {
              cat(sprintf("\n    %s → %s", rule_name, rules[[rule_name]]))
            }
          }
          
          # Get placeholder value safely
          null_placeholder <- tryCatch({
            as.numeric(rules[[1]])
          }, error = function(e) {
            if(verbose) {
              cat(sprintf("\n  Warning: Could not convert placeholder to numeric"))
              cat(sprintf("\n    Error: %s", e$message))
            }
            NA
          })
          
          if(verbose) {
            cat(sprintf("\n  Using placeholder: %s", 
                        if(is.na(null_placeholder)) "NA" else null_placeholder))
          }
          
          # Store original values
          orig_values <- unique(df[[field_name]])
          
          # Convert field to character first
          df[[field_name]] <- as.character(df[[field_name]])
          
          # Apply null transformations
          null_mask <- df[[field_name]] %in% c("null", "NaN", "") | is.na(df[[field_name]])
          null_count <- sum(null_mask)
          df[[field_name]][null_mask] <- null_placeholder
          
          # Apply type conversion if needed
          if (type %in% c("Integer", "Float")) {
            if(verbose) cat(sprintf("\n  Converting to %s", type))
            
            if (type == "Integer") {
              df[[field_name]] <- as.integer(df[[field_name]])
            } else {
              df[[field_name]] <- as.numeric(df[[field_name]])
            }
          }
          
          # Store transformation summary
          transform_summary[[field_name]] <- list(
            type = type,
            nulls_transformed = null_count,
            values_before = orig_values,
            values_after = unique(df[[field_name]])
          )
          
          if(verbose && null_count > 0) {
            cat(sprintf("\n  Transformed %d null values", null_count))
            cat("\n  Value comparison:")
            cat(sprintf("\n    Before: %s", 
                        paste(head(orig_values), collapse=", ")))
            cat(sprintf("\n    After:  %s", 
                        paste(head(unique(df[[field_name]])), collapse=", ")))
          }
        }
      }, error = function(e) {
        if(verbose) {
          cat(sprintf("\n\nError processing field %s:", field_name))
          cat(sprintf("\n  %s", e$message))
        }
      })
    }
  }
  
  if(verbose && length(transform_summary) > 0) {
    cat("\n\nNull transformation summary:")
    for(field in names(transform_summary)) {
      cat(sprintf("\n- %s", field))
      if(transform_summary[[field]]$nulls_transformed > 0) {
        cat(sprintf(" (%d nulls transformed)", 
                    transform_summary[[field]]$nulls_transformed))
      }
    }
    cat("\n")
  }
  
  return(df)
}

================
File: R/processCaprData.R
================
#' Process CAPR data
#' 
#' @param df Data frame containing CAPR data
#' @param instrument_name Name of the instrument
#' @return Processed data frame
#' @importFrom dplyr filter select mutate group_by first contains
#' @importFrom rlang .data
#' @noRd
processCaprData <- function(df, instrument_name) {
  # Convert and filter src_subject_id
  df$src_subject_id <- as.numeric(df$src_subject_id)
  # df <- filter(df, between(df$src_subject_id, 10000, 71110)) # between seems() to cause error
  # might be less flexible character to numeric
  # src_subject_id may download as character sometimes
  df <- dplyr::filter(df, .data$src_subject_id > 10000, .data$src_subject_id < 79110)
  # include guard clauses for mesaures that require aditional filtering beyond form name
  if (instrument_name == "scid_scoresheet") {
    df <- df %>% dplyr::select(contains(c("src_subject_id", "redcap_event_name", "scid_", "scip_", "mdd_", "pdd_"))) # scid_p18a was misspelled in the dataframe, that is why there is a "scip" variable :)
  }
  df$src_subject_id <- as.character(df$src_subject_id)
  
  # create a visit variable based on redcap_event_name
  ## not over-writing with rename(), so that redcap_event_name can do a "soft retire"
  df <- df %>% dplyr::mutate(visit = .data$redcap_event_name)
  
  # align redcap_event_name-ing convention with more natural language
  df <- df %>% dplyr::mutate(visit = ifelse(.data$visit == "baseline_arm_1", "bl",
                                            ifelse(.data$visit == "12m_arm_1", "12m",
                                                   ifelse(.data$visit == "24m_arm_1", "24m", NA)
                                            )
  ))
  
  # recode phenotype (only need to recode phenotypes as 4 (ineligible) and 5 (withdrawn) have been removed in previous line)
  df <- df %>% dplyr::mutate(phenotype = ifelse(is.na(.data$phenotype), NA,
                                                ifelse(.data$phenotype == 1, "hc",
                                                       ifelse(.data$phenotype == 2, "chr",
                                                              ifelse(.data$phenotype == 3, "hsc", 
                                                                     ifelse(.data$phenotype == 4, "ineligible",
                                                                            ifelse(.data$phenotype == 5, "withdrawn", NA)))))))
  
  #make sure phenotype doesn't change after baseline visit
  df <- df %>% 
    dplyr::mutate(visit = factor(.data$visit, levels = c('bl','12m','24m')),
                  phenotype = factor(.data$phenotype)) %>%
    dplyr::group_by(.data$src_subject_id) %>%  
    dplyr::mutate(baseline_pheno = dplyr::first(.data$phenotype)) %>% 
    dplyr::mutate(phenotype = .data$baseline_pheno) %>% 
    dplyr::select(-.data$baseline_pheno) 
  
  # Remove rows where phenotype is NA
  # but first print warning and say how many folks are getting removed
  phenotype_nas <- df[is.na(df$phenotype),]
  print(paste0('removing ', nrow(phenotype_nas),
               ' subjects because they have NA for phenotype. This generally',
               ' should not happen. Below are the subject IDs and visit dates ',
               'for these people. They should be inspected and fixed in redcap'))
  print(paste0(phenotype_nas$src_subject_id, ' ', phenotype_nas$visit))
  df <- df[!is.na(df$phenotype), ]
  
  # Remove rows where phenotype is 'ineligible' or 'withdrawn'
  df <- df[!(df$phenotype %in% c("ineligible", "withdrawn")), ]
  
  # create a site variable based on src_institution_name
  ## not over-writing with rename(), so that redcap_event_name can do a "soft retire"
  df <- df %>% dplyr::mutate(site = .data$src_institution_id)
  
  # get rid of deprecated variable names is good practice
  df <- subset(df, select = -src_institution_id)
  
  # convert dates
  df$int_diff <- as.numeric(df$int_end - df$int_start)
  df$interview_date <- df$int_start
  
  # remove dob
  df <- subset(df, select = -subject_dob)
  
  return(df)
}

================
File: R/SecretsEnv.R
================
# First, install R6 if you don't have it
if (!require(R6)) install.packages("R6"); library(R6)

#' Configuration Environment Class
#' 
#' @importFrom R6 R6Class
#' @noRd
SecretsEnv <- R6::R6Class("SecretsEnv",
                          public = list(
                            config_specs = list(
                              redcap = list(
                                required = c("uri", "token"),
                                types = c(uri = "character", token = "character")
                              ),
                              mongo = list(
                                required = c("connectionString"),
                                types = c(connectionString = "character")
                              ),
                              qualtrics = list(
                                required = c("apiKeys", "baseUrls"),
                                types = c(apiKeys = "vector", baseUrls = "vector")
                              ),
                              sql = list(
                                required = c("conn"),
                                types = c(conn = "character")
                              )
                            ),
                            
                            # Store the secrets file path
                            secrets_file = NULL,
                            
                            initialize = function(secrets_file = "secrets.R") {
                              # Check if secrets file exists
                              if (!file.exists(secrets_file)) {
                                stop(secrets_file, " file not found. Please create this file and define the required API variables.")
                              }
                              
                              # Store the secrets file path
                              self$secrets_file <- secrets_file
                              
                              # Source the secrets file
                              base::source(secrets_file)
                            },
                            
                            validate_config = function(api_type) {
                              if (!api_type %in% names(self$config_specs)) {
                                stop("Unknown API type: '", api_type, "'. Valid types are: ",
                                     base::paste(names(self$config_specs), collapse=", "))
                              }
                              
                              specs <- self$config_specs[[api_type]]
                              all_errors <- c()
                              
                              # Check that required variables exist
                              missing_vars <- specs$required[!base::sapply(specs$required, exists)]
                              if (length(missing_vars) > 0) {
                                all_errors <- c(all_errors, paste("Missing variables:",
                                                                  base::paste(missing_vars, collapse=", ")))
                              }
                              
                              # Only check existing variables for type and emptiness
                              existing_vars <- specs$required[base::sapply(specs$required, exists)]
                              
                              # Check variable types and emptiness for variables that exist
                              for (type_name in unique(specs$types)) {
                                # Get variables expected to be of this type that exist
                                vars_of_type_names <- names(specs$types[specs$types == type_name])
                                vars_of_type <- vars_of_type_names[vars_of_type_names %in% existing_vars]
                                
                                if (length(vars_of_type) > 0) {
                                  # Check which ones fail the type check
                                  failing_vars <- vars_of_type[base::sapply(vars_of_type, function(var) {
                                    var_value <- base::get(var)
                                    
                                    # Check the appropriate type
                                    !switch(type_name,
                                            "character" = is.character(var_value),
                                            "vector" = is.vector(var_value),
                                            FALSE  # Default case for unknown types
                                    )
                                  })]
                                  
                                  # Report if any variables fail this type check
                                  if (length(failing_vars) > 0) {
                                    if (type_name == "character") {
                                      all_errors <- c(all_errors, paste("Type error:",
                                                                        base::paste(failing_vars, collapse=", "),
                                                                        "must be defined as character strings using quotes."))
                                    } else if (type_name == "vector") {
                                      all_errors <- c(all_errors, paste("Type error:",
                                                                        base::paste(failing_vars, collapse=", "),
                                                                        "must be defined as vectors using c() function."))
                                    }
                                  }
                                  
                                  # Only check emptiness for variables of the correct type
                                  correct_type_vars <- vars_of_type[!vars_of_type %in% failing_vars]
                                  
                                  # For character variables, check for empty strings
                                  if (type_name == "character" && length(correct_type_vars) > 0) {
                                    empty_vars <- correct_type_vars[base::sapply(correct_type_vars, function(var) {
                                      var_value <- base::get(var)
                                      nchar(var_value) == 0 || all(trimws(var_value) == "")
                                    })]
                                    
                                    if (length(empty_vars) > 0) {
                                      all_errors <- c(all_errors, paste("Empty value error:",
                                                                        base::paste(empty_vars, collapse=", "),
                                                                        "cannot be empty strings."))
                                    }
                                  }
                                  
                                  # For vector variables, check if they're empty
                                  if (type_name == "vector" && length(correct_type_vars) > 0) {
                                    empty_vars <- correct_type_vars[base::sapply(correct_type_vars, function(var) {
                                      var_value <- base::get(var)
                                      length(var_value) == 0
                                    })]
                                    
                                    if (length(empty_vars) > 0) {
                                      all_errors <- c(all_errors, paste("Empty vector error:",
                                                                        base::paste(empty_vars, collapse=", "),
                                                                        "cannot be empty vectors."))
                                    }
                                  }
                                  
                                  # For REDCap URI, check trailing slash and 'api' endpoint
                                  if (api_type == "redcap" && "uri" %in% correct_type_vars) {
                                    uri_value <- base::get("uri")
                                    
                                    # Check if it ends with "api/" (correct format)
                                    if (grepl("api/$", uri_value)) {
                                      # Format is correct, do nothing
                                    } 
                                    # Check if it ends with "api" (missing trailing slash)
                                    else if (grepl("api$", uri_value)) {
                                      # Add trailing slash
                                      fixed_uri <- paste0(uri_value, "/")
                                      
                                      # Update the variable in memory
                                      # assign("uri", fixed_uri, envir = .GlobalEnv)
                                      assign("uri", fixed_uri, envir = .wizaRdry_env)
                                      
                                      # Update the secrets.R file
                                      if (file.exists(self$secrets_file)) {
                                        # Read and update file content
                                        file_content <- readLines(self$secrets_file)
                                        uri_pattern <- "^\\s*uri\\s*<-\\s*[\"\'](.*)[\"\']\\s*$"
                                        uri_line_index <- grep(uri_pattern, file_content)
                                        
                                        if (length(uri_line_index) > 0) {
                                          file_content[uri_line_index] <- gsub(uri_pattern,
                                                                               paste0("uri <- \"", fixed_uri, "\""),
                                                                               file_content[uri_line_index])
                                          writeLines(file_content, self$secrets_file)
                                          message("Note: Added trailing slash to uri in ", self$secrets_file,
                                            " (", uri_value, " -> ", fixed_uri, ")")
                                        } else {
                                          message("Note: Added trailing slash to uri in memory, but couldn't update ",
                                                  self$secrets_file, " automatically.")
                                        }
                                      }
                                    } 
                                    # Not ending with "api" at all
                                    else {
                                      # Don't add a slash, add an error instead
                                      all_errors <- c(all_errors, paste("URI format error:",
                                                                        "uri must end with 'api/' for REDCap API access.",
                                                                        "Please verify REDCap API endpoint in the API Playground"))
                                    }
                                  }
                                  
                                  # Check REDCap token length
                                  if (api_type == "redcap" && "token" %in% correct_type_vars) {
                                    token_value <- base::get("token")
                                    if (nchar(token_value) < 32) {
                                      all_errors <- c(all_errors, paste("Token length error:",
                                                                        "token must be at least 32 characters long for REDCap API."))
                                    }
                                  }
                                }
                              }
                              
                              # If we found any errors, report them all at once
                              if (length(all_errors) > 0) {
                                stop(api_type, " API configuration errors in secrets.R:\n- ",
                                     paste(all_errors, collapse="\n- "), call. = FALSE)
                              } else {
                                # message("All ", api_type, " API credentials in secrets.R are valid.")
                              }
                              
                              return(TRUE)
                            }
                          )
)

# Create a wrapper function to make validation easier
validate_secrets <- function(api_type, secrets_file = "secrets.R") {
  secrets <- SecretsEnv$new(secrets_file)
  return(secrets$validate_config(api_type))
}

================
File: R/testSuite.R
================
#' Run a Suite of Tests on Dataset
#'
#' This function performs a series of validation tests on a dataset to ensure it meets
#' quality standards and contains required variables for NDA submission.
#'
#' @param measure_alias Character string, the name of the measure to be tested
#' @param measure_type Character string, the type of measure (e.g., "redcap", "qualtrics", "mongo")
#' @param script_path Character string, the path to the script that processes the measure
#' @param super_key Character vector, one or more identifier variables to check for in the dataset
#'
#' @return No return value, called for side effects
#'
#' @details
#' The function performs the following checks:
#' \itemize{
#'   \item Checks for duplicate records in Qualtrics data
#'   \item Verifies that a cleaned data frame exists
#'   \item Checks that all NDA required variables are present
#'   \item Ensures column naming is consistent with measure prefix standards
#'   \item Validates that interview age is within acceptable range
#'   \item Verifies that super key variables are present in both raw and cleaned data
#' }
#'
#' @examples
#' \dontrun{
#'   testSuite("rgpts", "qualtrics", "./clean/qualtrics/rgpts.R", "src_subject_id")
#' }
#'
#' @importFrom utils head
#' @noRd
testSuite <- function(measure_alias, measure_type, script_path, super_key) {
#   # Source all scripts in ./api/test
#   lapply(list.files("api/test", pattern = "\\.R$", full.names = TRUE), base::source)
  
  # List of NDA required variables
  nda_required_variables <- c("src_subject_id", "phenotype", "site", "arm", "visit", "week", 
                              "subjectkey", "sex", "interview_date", "interview_age", "state")
  
  # Perform checks
  checkQualtricsDuplicates(measure_alias, measure_type) # and give allow to View them in a table
  cleanDataFrameExists(measure_alias, measure_type) #checkin_clean x
  ndaRequiredVariablesExist(measure_alias, measure_type, nda_required_variables) # do Nda req variables exist
  checkColumnPrefix(measure_alias, measure_type, nda_required_variables) # checkin_distress
  checkInterviewAge(measure_alias) # <240 >860
  
  
  if (exists(measure_alias, envir = .GlobalEnv)) {
    print("Raw data found. Looking for super keys...")
    # Check for presence of super_key variables in the raw data
    candidate_keys <- checkKeys(measure_alias, super_key, "raw")
    # Assuming you still want to verify these keys are present in the cleaned data
    if (exists(paste0(measure_alias, "_clean"), envir = .GlobalEnv)) {
      print("Clean data found. Looking for candidate keys... ")
      candidate_keys <- checkKeys(paste0(measure_alias, "_clean"), candidate_keys, "clean")
    }
  }
  
  
  # ...add additional functions here, making sure they pass in measure_alias and measure_type
}

================
File: R/zzz.R
================
#' @importFrom utils capture.output head install.packages setTxtProgressBar txtProgressBar
NULL

.wizaRdry_env <- new.env(parent = emptyenv())

================
File: .gitignore
================
.Rproj.user
.Rhistory
.RData
.Ruserdata
secrets.R
config.yml

================
File: .Rbuildignore
================
^dataWizardry\.Rproj$
^\.Rproj\.user$
^LICENSE\.md$

================
File: dataWizardry.Rproj
================
Version: 1.0

RestoreWorkspace: No
SaveWorkspace: No
AlwaysSaveHistory: Default

EnableCodeIndexing: Yes
UseSpacesForTab: Yes
NumSpacesForTab: 2
Encoding: UTF-8

RnwWeave: Sweave
LaTeX: pdfLaTeX

AutoAppendNewline: Yes
StripTrailingWhitespace: Yes
LineEndingConversion: Posix

BuildType: Package
PackageUseDevtools: Yes
PackageInstallArgs: --no-multiarch --with-keep.source
PackageRoxygenize: rd,collate,namespace

================
File: DESCRIPTION
================
Package: wizaRdry
Title: A Framework For Collaborative & Reproducable Data Analysis
Version: 0.0.0.9000
Authors@R: c(
    person("Joshua", "Kenney", email = "joshua.kenney@yale.edu", role = c("aut", "cre")),
    person("Minerva", "Pappu", email = "minerva.pappu@yale.edu", role = "aut"),
    person("Trevor", "Williams", email = "trevormsu@gmail.com", role = "aut"),
    person("Victor", "Pokorny", email = "victor.pokorny@northwestern.edu", role = "ctb"),
    person("Christian", "Horgan", email = "christian.horgan@yale.edu", role = "ctb"),
    person("Michael", "Spilka", email = "michael.j.spilka@gmail.com", role = "ctb")
  )
Description: The wizaRdry package is a powerful data integration framework that solves the fundamental problem of inconsistent access to research data across multiple platforms. It eliminates repetitive boilerplate code by providing a unified interface to REDCap, MongoDB, and Qualtrics through its core functions: getRedcap(), getMongo(), and getQualtrics(). The package implements memory-aware parallel processing that automatically scales to available system resources, robust configuration validation, and comprehensive error handling with intelligent fallback strategies. By standardizing data retrieval across platforms and automating field harmonization, wizaRdry significantly reduces the risk of inconsistencies in multi-researcher environments. This package is essential for any research team working with distributed data sources who need reproducible, validated access to their research data.
License: MIT + file LICENSE
Encoding: UTF-8
Roxygen: list(markdown = TRUE)
RoxygenNote: 7.3.2
Imports: 
    cli,
    config,
    dplyr,
    future,
    future.apply,
    httr,
    jsonlite,
    knitr,
    mongolite,
    parallel,
    qualtRics,
    R6,
    REDCapR,
    rlang,
    stringdist

================
File: LICENSE
================
YEAR: 2025
COPYRIGHT HOLDER: dataWizardry authors

================
File: LICENSE.md
================
# MIT License

Copyright (c) 2025 dataWizardry authors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: NAMESPACE
================
# Generated by roxygen2: do not edit by hand

export(checkColumnPrefix)
export(checkInterviewAge)
export(checkQualtricsDuplicates)
export(cleanDataFrameExists)
export(dataRequest)
export(getMongo)
export(getQualtrics)
export(getRedcap)
export(getSurvey)
export(getTask)
export(ndaCheckQualtricsDuplicates)
export(ndaRequest)
export(ndaRequiredVariablesExist)
import(dplyr)
import(httr)
import(jsonlite)
import(qualtRics)
importFrom(R6,R6Class)
importFrom(REDCapR,redcap_instruments)
importFrom(REDCapR,redcap_metadata_read)
importFrom(REDCapR,redcap_read)
importFrom(base,get)
importFrom(base,setdiff)
importFrom(cli,console_width)
importFrom(config,get)
importFrom(dplyr,"%>%")
importFrom(dplyr,bind_rows)
importFrom(dplyr,contains)
importFrom(dplyr,filter)
importFrom(dplyr,first)
importFrom(dplyr,group_by)
importFrom(dplyr,mutate)
importFrom(dplyr,select)
importFrom(dplyr,setdiff)
importFrom(future,future)
importFrom(future,multisession)
importFrom(future,plan)
importFrom(future.apply,future_lapply)
importFrom(knitr,kable)
importFrom(mongolite,mongo)
importFrom(mongolite,ssl_options)
importFrom(parallel,detectCores)
importFrom(qualtRics,fetch_survey)
importFrom(rlang,.data)
importFrom(stats,setNames)
importFrom(stringdist,stringdist)
importFrom(testthat,expect_true)
importFrom(testthat,test_that)
importFrom(utils,capture.output)
importFrom(utils,flush.console)
importFrom(utils,head)
importFrom(utils,install.packages)
importFrom(utils,setTxtProgressBar)
importFrom(utils,txtProgressBar)

================
File: README.md
================
# dataWizardry

<!-- badges: start -->
<!-- badges: end -->

The goal of dataWizardry is to ...

## Installation

You can install the development version of dataWizardry from [GitHub](https://github.com/) with:

``` r
# install.packages("pak")
pak::pak("belieflab/dataWizardry")
```

## Example

This is a basic example which shows you how to solve a common problem:

``` r
library(dataWizardry)
## basic example code
```
